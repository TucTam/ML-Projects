{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f63c96e-15b7-437d-9bff-96793672f5ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55a045c09fd44e566360fd4149c996c2",
     "grade": false,
     "grade_id": "cell-8daf3867610b445d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Important! Please do not remove any cells, including the test cells, even if they appear empty. They contain hidden tests, and deleting them could result in a loss of points, as the exercises are graded automatically. Only edit the cells where you are instructed to write your solution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f128eda",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2d0db7c4138413f972b2bb68affaa23",
     "grade": false,
     "grade_id": "cell-4fefb1f0dee49d3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Exercise 1\n",
    "## Part 2. NumPy Implementation for Network Training\n",
    "\n",
    "In the second part of this assignment, you will implement the same network given in the pen-and-paper task using **NumPy**. \n",
    "\n",
    "### Objective\n",
    "\n",
    "Your task is to work with a class-based structure to represent the key components of the neural network. You will be given a template code for each of these components. You are expected to implement the necessary parts step by step. The goal is to help you get familiar with the fundamental building blocks of a neural network and understand how they are formed together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89cd07c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a518c426b6019bd60105e21b2221bf18",
     "grade": false,
     "grade_id": "cell-61d9f24eac121db2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You will implement and train the given computational graph by following these steps:\n",
    "1. Linear layer: implementation of backward and forward passes and parameter update\n",
    "2. Tanh activation function: implementation of backward and forward passes\n",
    "3. Multilayer Perceptron (MLP): building the model architecture by combining Linear and Tanh layers\n",
    "4. Mean Squared Error Loss: computation of the loss and its gradients with respect to the model's output\n",
    "5. Training loop: no implementation is required, you will observe the model's training progress.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <figure>\n",
    "        <img src=\"ex1_graph_01-3.png\" alt=\"Image Title\" style=\"width:900px; height:auto;\"/>\n",
    "        <figcaption>Figure 1: Forward and Backward Pass in the MLP</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "\n",
    "##### Important Note:\n",
    "1. In the implementation of the components, it is important that the forward and backward passes operate on the same data. Since the forward pass is **always** executed first, we store the necessary input data as class attributes during the forward pass. In this way, we ensure that the same data is accessed during the backward pass without any changes between the two passes. For instance, the input $x$ that is used in both the forward and backward pass of the Linear layer will be passed as an argument to the `forward()` method and stored as a class attribute (e.g. `self.x`) to be accessed in the `backward()` method. You will notice this practice repeated in the implementation of different components.\n",
    "2. Throughout the assignment, you are expected to implement the sections marked as:\n",
    "   ```python\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "   ```\n",
    "   In some parts of the code, certain variables are initialized as `None` (e.g., self.grad_weights = None). These are placeholders to guide you on which steps are expected from you. You should overwrite these `None` values with the correct computations as part of your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a712af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8611e62e9dd958ada01cb7d0b5f51b9e",
     "grade": false,
     "grade_id": "cell-6145f9065a46d40b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17ca3d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12ec913cc1dfaca927716098a2800653",
     "grade": false,
     "grade_id": "cell-a816492239a1ccff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382408d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9364cac24399323fc624ecb1c3cb6de3",
     "grade": false,
     "grade_id": "cell-75bdb0ce1e94ccfd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 1. Linear layer\n",
    "You are given a template code of the Linear class with the following methods:\n",
    "- `__init__()` to initialize the weights and biases. \n",
    "- `forward()` to handle the forward pass.\n",
    "- `backward()` to handle the backward pass.\n",
    "- `update_params()` to update the weights and biases using the calculated gradients.\n",
    " \n",
    "Start by implementing the forward pass, then proceed with the backward pass, and finally implement the parameter updates.\n",
    "\n",
    "##### Steps to follow:\n",
    "##### 1. Forward pass: \n",
    "Compute the layer's output $y = W x + b$, where $W$ and $b$ are the weight matrix and bias, and $x$ is the input to the linear layer.\n",
    "##### 2. Backward pass: \n",
    "You need to compute the gradient of the loss with respect to the layer's **input**, **weights**, and **biases**.  The `backward()` method receives the argument `grad_output`, which represents the gradient of the loss with respect to this layer's output, coming from the next layer in the network.  You need to use `grad_output` in your calculations when computing the gradients of the loss. You can refer to the computational graph in Figure 1 to understand the flow of gradients through the network.\n",
    "##### 3. Update parameters:\n",
    "Use the computed gradients to update the parameters (weights and biases for the next iteration) with the given `learning_rate`.\n",
    "##### Hints:\n",
    "1. **Matrix shapes:** Make sure that the shapes of your input, weights, and biases are compatible during the matrix multiplications.  You are expected to implement $y = W x + b$ for a **batch of inputs**. For a single input sample, _x_ is typically a column vector, but when dealing with multiple input samples, the input matrix will contain one row for each input sample. Pay attention to the shapes provided in the docstring of each method, and apply the transpose where necessary.\n",
    "2. **Backward pass gradients:** During the backward pass, you need to return only the gradient of the loss with respect to the input (`self.grad_input`). This is necessary because it will be passed to the preceding layers as part of the chain rule during backpropagation. However, you also need to compute `self.grad_weight` and `self.grad_bias`. These gradients will not be returned as they do not contribute to the chain rule for the coming layers, but they will be used internally during the parameter update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc130b3",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36554908cf941f36650afa9e4ea94486",
     "grade": false,
     "grade_id": "cell-331438facc0f32cb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_dim, output_dim, initial_weights=None, initial_biases=None):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases\n",
    "        \n",
    "        Args:\n",
    "        - input_dim (int): Number of input features.\n",
    "        - output_dim (int): Number of output features.\n",
    "        - initial_weights (np.array): Initial weights of shape (output_dim, input_dim).\n",
    "        - initial_biases (np.array): Initial biases of shape (output_dim,).\n",
    "        \"\"\"\n",
    "        if initial_weights is None: initial_weights = np.random.randn(output_dim, input_dim)\n",
    "        if initial_biases is None: initial_biases = np.random.randn(output_dim)\n",
    "        self.weights = initial_weights\n",
    "        self.biases = initial_biases\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the linear transformation\n",
    "        \n",
    "        Args:\n",
    "        - x (np.array): Input data of shape (num_samples, input_dim).\n",
    "        \n",
    "        Returns:\n",
    "        - output (np.array): Output data of shape (num_samples, output_dim).\n",
    "        \"\"\"\n",
    "        self.x = x # Keep this to use in backward method\n",
    "        # Compute self.output \n",
    "        self.output = None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Perform the backward pass of the Linear layer.\n",
    "        \n",
    "        Args:\n",
    "        - grad_output (np.array): Gradient of the loss with respect to the output with shape (num_samples, output_dim).\n",
    "        \n",
    "        Returns:\n",
    "        - grad_input (np.array): Gradient of the loss with respect to the input with shape (num_samples, input_dim).\n",
    "        \"\"\"\n",
    "        # Compute self.grad_weights, self.grad_biases, self.grad_input \n",
    "        assert hasattr(self, 'x'), 'Perform forward pass first.'\n",
    "        self.grad_weights = None\n",
    "        self.grad_biases = None\n",
    "        self.grad_input  = None\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return self.grad_input\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights and biases using the calculated gradients.\n",
    "        \n",
    "        Args:\n",
    "        - learning_rate (float): Learning rate for updating parameters.\n",
    "        \"\"\"\n",
    "        # Update self.weights and self.biases\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa697a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "979e532dbcadc409da175a2a3eda1e80",
     "grade": true,
     "grade_id": "cell-4f220513dd5dfd44",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_linear_calls():\n",
    "    num_samples, input_dim, output_dim = 5, 3, 4\n",
    "    \n",
    "    x_dummy = np.random.randn(num_samples, input_dim)\n",
    "    w_dummy = np.random.randn(output_dim, input_dim)\n",
    "    b_dummy = np.random.randn(output_dim)\n",
    "    grad_output_dummy = np.random.randn(num_samples, output_dim)\n",
    "    learning_rate = 0.5\n",
    "\n",
    "    # Forward pass\n",
    "    linear_layer = Linear(input_dim=input_dim, output_dim=output_dim, initial_weights=w_dummy.copy(), initial_biases=b_dummy.copy())\n",
    "    output = linear_layer.forward(x_dummy)    \n",
    "    assert output is not None, 'Forward output is not implemented or set as None.'\n",
    "    assert output.shape == (num_samples, output_dim), f'Expected output shape: {(num_samples, output_dim)}, but got: {output.shape}'\n",
    "\n",
    "    # Backward pass\n",
    "    grad_input = linear_layer.backward(grad_output_dummy)\n",
    "    assert grad_input is not None, 'Backward pass returned None for grad_input.'\n",
    "    assert grad_input.shape == (num_samples, input_dim), f'Expected grad_input shape: {(num_samples, input_dim)}, but got: {grad_input.shape}'\n",
    "    \n",
    "    assert linear_layer.grad_weights is not None, 'grad_weights is not implemented or set as None.'\n",
    "    assert linear_layer.grad_weights.shape == (output_dim, input_dim), f'Expected grad_weights shape: {(output_dim, input_dim)}, but got: {linear_layer.grad_weights.shape}'\n",
    "    \n",
    "    assert linear_layer.grad_biases is not None, 'grad_biases is not implemented or set as None.'    \n",
    "    assert linear_layer.grad_biases.shape == (output_dim,), f'Expected grad_biases shape: {(output_dim,)}, but got: {linear_layer.grad_biases.shape}'\n",
    "\n",
    "    # Save the current weights and biases for comparison\n",
    "    old_weights = linear_layer.weights.copy()\n",
    "    old_biases = linear_layer.biases.copy()\n",
    "    \n",
    "    # Update parameters\n",
    "    linear_layer.update_params(learning_rate)\n",
    "\n",
    "    # Ensure weights and biases are updated (not the same as before)\n",
    "    assert not np.allclose(linear_layer.weights, old_weights), 'Weights were not updated correctly.'\n",
    "    assert not np.allclose(linear_layer.biases, old_biases), 'Biases were not updated correctly.'\n",
    "    \n",
    "    assert linear_layer.weights.shape == (output_dim, input_dim), f'Expected weights shape: {(output_dim, input_dim)}, but got: {linear_layer.weights.shape}'\n",
    "    assert linear_layer.biases.shape == (output_dim,), f'Expected biases shape: {(output_dim,)}, but got: {linear_layer.biases.shape}'\n",
    "    \n",
    "    print('Visible tests for linear layer passed successfully!')\n",
    "\n",
    "test_linear_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d2f201-09d2-4b1b-ad0f-6e13f1c876b2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfbbca58311a1507385dd1c894513205",
     "grade": false,
     "grade_id": "cell-b8975d50392302cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In order to check your `backward()` method, you can validate it by comparing your computation with gradients computed via [numerical differentiation](https://en.wikipedia.org/wiki/Numerical_differentiation) in the form: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} â‰ˆ \\frac{f(x+ \\epsilon) - f(x-\\epsilon)}{2\\epsilon},\n",
    "$$\n",
    "\n",
    "where $f(x)$ is a function of the input vector $x$, and $\\epsilon$ is a small deviation value.\n",
    "\n",
    "In the code below, we first define a utility function `compute_numerical_gradient()` to compute the gradient of the forward pass of a layer with respect to its input. \n",
    "\n",
    "##### Note:\n",
    "- Although this function calculates the gradient of a **layer's output** with respect to its input, remember that in our implementation, we are also using the gradient from subsequent layers (i.e., `grad_output`) to handle the chain rule in each layer. So, make sure to correctly handle `grad_output` in your implementation to compute the gradient of the **loss** with respect to the layer's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf167d1-c149-4842-af96-130a5a52c46a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb67a6a7459a060fb7c0d3dde148472b",
     "grade": false,
     "grade_id": "cell-5e96e72cdb8438f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_numerical_gradient(layer, x, eps=1e-4):\n",
    "    \"\"\"\n",
    "    Compute the numerical gradient of the forward pass with respect to the input x.\n",
    "\n",
    "    Args:\n",
    "    - layer: The layer whose forward pass we are testing.\n",
    "    - x (np.array): Input data of shape (num_samples, num_features).\n",
    "    - eps (float): Small deviation value for numerical gradient calculation.\n",
    "\n",
    "    Returns:\n",
    "    - numerical_grad (np.array): The numerical gradient of shape (num_samples, num_features).\n",
    "    \"\"\"\n",
    "    assert hasattr(layer, 'forward'), 'layer must have a forward method'\n",
    "    assert x.ndim == 2, f'Expected 2D array x, but got {x.ndim}D'\n",
    "    num_samples, num_features = x.shape\n",
    "    numerical_grad = np.zeros_like(x)  # Initialize the gradient matrix\n",
    "\n",
    "    # Loop over each sample and feature\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_features):\n",
    "            # Create perturbed inputs\n",
    "            x_pos = x.copy()\n",
    "            x_neg = x.copy()\n",
    "            \n",
    "            x_pos[i, j] += eps  \n",
    "            x_neg[i, j] -= eps  \n",
    "            \n",
    "            # Compute the forward pass\n",
    "            y_pos = layer.forward(x_pos)\n",
    "            y_neg = layer.forward(x_neg)\n",
    "            \n",
    "            # Approximate the gradient using finite differences\n",
    "            numerical_grad[i, j] = (y_pos - y_neg).sum() / (2 * eps)\n",
    "\n",
    "    return numerical_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfea18-6f82-4a31-be40-7c85ee7d13ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "905d732139597a99bc00a64b2d156182",
     "grade": true,
     "grade_id": "cell-3a5198255386db49",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This checks if dy/dx works correctly.\n",
    "# Make sure that your backward function also considers grad_output to return dL/dx\n",
    "def test_gradients():\n",
    "    num_samples, input_dim, output_dim = 5, 3, 4\n",
    "    eps = 1e-4\n",
    "    x_dummy = np.random.randn(num_samples, input_dim)\n",
    "    w_dummy = np.random.randn(output_dim, input_dim)\n",
    "    b_dummy = np.random.randn(output_dim)\n",
    "    grad_output_dummy = np.ones((num_samples, output_dim)) # Dummy gradient for the output layer\n",
    "\n",
    "    linear_layer = Linear(input_dim=input_dim, output_dim=output_dim, \n",
    "                          initial_weights=w_dummy.copy(), initial_biases=b_dummy.copy())\n",
    "    \n",
    "    output = linear_layer.forward(x_dummy) # Forward pass     \n",
    "    analytical_grad = linear_layer.backward(grad_output_dummy) # Backward pass (your implementation)   \n",
    "    numerical_grad = compute_numerical_gradient(linear_layer, x_dummy) # Compute numerical gradient\n",
    "    \n",
    "    assert np.allclose(analytical_grad, numerical_grad, atol=1e-4), f'Gradients do not match. Analytical: {analytical_grad}, Numerical: {numerical_grad}'\n",
    "    print('Visible numerical gradient test passed successfully!')\n",
    "\n",
    "test_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c73350-2550-423e-8b9e-36ccf9cef694",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7664fa4e8b6683a213241d52f40d0eb",
     "grade": true,
     "grade_id": "cell-27741a23128b00af",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell contains hidden test cases that will be evaluated after submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8061e1-e9d0-4413-8311-cc56ad0226a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "802bf92b4096136693bf124c31dbd2c7",
     "grade": true,
     "grade_id": "cell-0ba32dc4f9690e28",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell contains hidden test cases that will be evaluated after submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee673a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d327c9d30bb020e7089b516e62d1860",
     "grade": true,
     "grade_id": "cell-62bd3a79cce9ba75",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell contains hidden test cases that will be evaluated after submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377662d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0696d8d4edf0d582513e7bafbf6182f9",
     "grade": false,
     "grade_id": "cell-70d220f269b27b64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2. Tanh Activation Function\n",
    "You are given a template code of the **Tanh** class with the following methods:\n",
    "- `forward()` to apply the Tanh activation function in the forward pass.\n",
    "- `backward()` to compute the gradient of the loss with respect to the input.\n",
    " \n",
    "The hyperbolic tangent (Tanh) is defined as: $\\text{tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$.\n",
    "\n",
    "##### Steps to follow:\n",
    "##### 1. Forward pass: \n",
    "Apply the Tanh activation function on the given input data `x` to the function. You can use [NumPy's tanh function](https://numpy.org/doc/2.0/reference/generated/numpy.tanh.html).\n",
    "##### 2. Backward pass: \n",
    "Use the chain rule along with the derivative of Tanh $\\frac{\\partial}{\\partial x} \\text{tanh}(x)$. Combine this with the incoming `grad_output` from its following layers to compute the gradient with respect to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe37c8",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4556e2cf88d578bfd116b4dca44e6013",
     "grade": false,
     "grade_id": "cell-8314a55136701073",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply the Tanh activation function.\n",
    "        \n",
    "        Args:\n",
    "        - x (np.array): Input data of shape (num_samples, input_dim).\n",
    "        \n",
    "        Returns:\n",
    "        - output (np.array): Activated output data of shape (num_samples, input_dim).\n",
    "        \"\"\"\n",
    "        self.x = x # Keep this for backward computation\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the loss with respect to the input of Tanh.\n",
    "        \n",
    "        Args:\n",
    "        - grad_output (np.array): Gradient of the loss with respect to the output.\n",
    "        \n",
    "        Returns:\n",
    "        - grad_input (np.array): Gradient of the loss with respect to the input.\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'x'), 'Perform forward pass first.'\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return self.grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3003f-dcd9-45c6-b1f0-319928aa9fe1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0cc82a31bd571add49a6d9fca493ea4",
     "grade": true,
     "grade_id": "cell-4ba5d59aefb8d9c5",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_tanh_shapes():\n",
    "    num_samples, input_dim = 3,4\n",
    "    x_dummy = np.random.randn(num_samples,input_dim)\n",
    "    tanh_layer = Tanh()\n",
    "    y_dummy = tanh_layer.forward(x_dummy)\n",
    "    grad_output = np.random.randn(num_samples, input_dim)\n",
    "    grad_input = tanh_layer.backward(grad_output)\n",
    "    assert grad_input.shape == x_dummy.shape, f'Expected grad_input shape {x_dummy.shape} but got {grad_input.shape}'\n",
    "    print('Visible shape test for tanh passed successfully!')\n",
    "\n",
    "test_tanh_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6934b6-4f3a-4031-b55c-9681a0fd1e97",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0ac79935f3fbbdee1a15d67580c99f0",
     "grade": true,
     "grade_id": "cell-c805582f1082ca2a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This checks if d(tanh)/dx works correctly.\n",
    "# Make sure that your backward function also considers grad_output - coming back from subsequent layers\n",
    "def test_gradients():\n",
    "    num_samples, input_dim = 3,4\n",
    "    eps = 1e-4\n",
    "    x_dummy = np.random.randn(num_samples,input_dim)\n",
    "    tanh_layer = Tanh()\n",
    "    \n",
    "    grad_output_dummy = np.ones((num_samples, input_dim)) # Dummy gradient for the output layer\n",
    "    output = tanh_layer.forward(x_dummy) # Forward pass     \n",
    "    analytical_grad = tanh_layer.backward(grad_output_dummy) # Backward pass (your implementation)   \n",
    "    numerical_grad = compute_numerical_gradient(tanh_layer, x_dummy) # Compute numerical gradient\n",
    "\n",
    "    assert np.allclose(analytical_grad, numerical_grad, atol=1e-4), f'Gradients do not match. Analytical: {analytical_grad}, Numerical: {numerical_grad}'\n",
    "    print('Visible numerical gradient test passed successfully!')\n",
    "\n",
    "test_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23371e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba5e827256d565073a6e70543f21f6d5",
     "grade": true,
     "grade_id": "cell-7c59db2dfe9b4603",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell contains hidden test cases that will be evaluated after submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d25fbfb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c55da61336d8c73a268d2dde48373dfc",
     "grade": false,
     "grade_id": "cell-6e9fc54b12d4359c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3. Multilayer Perceptron (MLP)\n",
    "\n",
    "In this step, we will combine the Linear layer and Tanh activation to build the complete model architecture. The MLP class will have two layers and Tanh activation in between.\n",
    "\n",
    "You are given a template code of the **MLP** class using instances of **Linear** and **Tanh** classes. The layers for the model architecture are already initialized and the forward pass is given. Your task is to implement the **backward pass** and **parameter updates**. \n",
    "\n",
    "##### Steps to follow:\n",
    "1. Backward pass. You will propagate the gradients in reverse order:\n",
    "    - Use the `grad_output` to compute the gradients for the second linear layer (output layer).\n",
    "    - Propagate these gradients through Tanh activation function.\n",
    "    - Propagate the gradients through the first Linear layer.\n",
    "2. You also need to update the parameters of each linear layer.\n",
    "   \n",
    "You will use the backward() methods of the Linear and Tanh classes and update_params() of the Linear class that you implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba46841",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18cb38b6a8cd7b952423ee4def39477c",
     "grade": false,
     "grade_id": "cell-71e8fc3e6004d212",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the MLP with the necessary layers.\n",
    "        \n",
    "        Args:\n",
    "        - input_dim (int): Number of input features.\n",
    "        - hidden_dim (int): Number of units in the hidden layer.\n",
    "        - output_dim (int): Number of units in the output layer.\n",
    "        \"\"\"\n",
    "        # Initialize the linear layers and activation function\n",
    "        self.linear1 = Linear(input_dim, hidden_dim)\n",
    "        self.activation = Tanh()\n",
    "        self.linear2 = Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the MLP.\n",
    "        \n",
    "        Args:\n",
    "        - x (np.array): Input data.\n",
    "        \n",
    "        Returns:\n",
    "        - output (np.array): Output of the MLP.\n",
    "        \"\"\"\n",
    "        hidden = self.linear1.forward(x)\n",
    "        activated_hidden = self.activation.forward(hidden) \n",
    "        output = self.linear2.forward(activated_hidden)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass through the MLP.\n",
    "        \n",
    "        Args:\n",
    "        - grad_output (np.array): Gradient of the loss with respect to the MLP output.\n",
    "        \n",
    "        Returns:\n",
    "        - grad_input (np.array): Gradient of the loss with respect to the MLP input.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return grad_input\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the parameters of the MLP.\n",
    "        \n",
    "        Args:\n",
    "        - learning_rate (float): Learning rate for parameter updates.\n",
    "        \"\"\"\n",
    "        # Update the parameters of each linear layer\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061cc9ed-5735-417f-aea0-ffec451fd449",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73aa3fd20b4dc0b3405c70bc13ac3aea",
     "grade": true,
     "grade_id": "cell-5ce404d73c520087",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test case to check the shape of gradient wrt input after backward pass\n",
    "def test_mlp_gradient_shapes():\n",
    "    input_dim, hidden_dim, output_dim = 3, 2, 1\n",
    "    num_samples = 5  # Test with a batch of 5 samples\n",
    "    x_dummy = np.random.randn(num_samples, input_dim)  # Input with shape (num_samples, input_dim)\n",
    "    grad_output_dummy = np.random.randn(num_samples, output_dim)  # Gradient of shape (num_samples, output_dim)\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # Initialize the MLP model\n",
    "    mlp = MLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = mlp.forward(x_dummy)\n",
    "\n",
    "    # Backward pass\n",
    "    grad_input = mlp.backward(grad_output_dummy)\n",
    "\n",
    "    assert grad_input.shape == x_dummy.shape, f'Expected grad_input shape {x_dummy.shape}, but got {grad_input.shape}'\n",
    "    \n",
    "    print('Visible shape test for MLP gradient wrt input passed.')\n",
    "\n",
    "test_mlp_gradient_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfee999",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80135fc7eb4eab06bed970397d1bcb38",
     "grade": true,
     "grade_id": "cell-d688093a32c745b5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell contains hidden test cases that will be evaluated after submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc99042",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8d802a9111b69c31451414996bff980",
     "grade": false,
     "grade_id": "cell-a818832ff742a49e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 4. Mean Squared Error Loss\n",
    "You are given a template code of the **MSELoss** class with the following methods:\n",
    "- `forward()` to compute the Mean Squared Error between the predicted output `y` and the target `t`.\n",
    "- `backward()` to compute the gradient of the loss with respect to the predicted output.\n",
    " \n",
    "The Mean Squared Error (MSE) loss is defined as: $L = \\frac{1}{N} \\sum_{i=1}^N (y_{i} - t_{i})^2$, where $y_{i}$ and $t_{i}$ are the predicted and target output for the $i^{th}$ data point and $N$ is the number of data points.\n",
    "\n",
    "##### Steps to follow:\n",
    "1. Forward pass: \n",
    "Apply the MSE Loss for the given predicted output and the true target.\n",
    "2. Backward pass: \n",
    "Compute the gradient of the loss with respect to the predicted output $\\frac{\\partial L}{\\partial y}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef7e822",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cdda62570621565b7ef587f552a3b0d",
     "grade": false,
     "grade_id": "cell-044d8fe576a0cc15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def forward(self, y, t):\n",
    "        \"\"\"\n",
    "        Compute the mean squared error loss.\n",
    "        \n",
    "        Args:\n",
    "        - y (np.array): Predicted values.\n",
    "        - t (np.array): True values.\n",
    "        \n",
    "        Returns:\n",
    "        - loss (float): Computed MSE loss.\n",
    "        \"\"\"        \n",
    "        # ! Store inputs as class attribute to prevent any changes between two passes\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the loss with respect to the predicted output.\n",
    "        \n",
    "        Returns:\n",
    "        - grad_input (np.array): Gradient of the loss with respect to the predicted output.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933fe92-0129-44d8-9a35-302e47ad699f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b51b411039ad512a620c4f75c717daba",
     "grade": true,
     "grade_id": "cell-f4d8f6ca7d047529",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_mse_loss_shapes():\n",
    "    num_samples, output_dim = 5, 3 \n",
    "    y_dummy = np.random.randn(num_samples, output_dim)\n",
    "    t_dummy = np.random.randn(num_samples, output_dim) \n",
    "    \n",
    "    mse_loss = MSELoss()\n",
    "\n",
    "    # Forward pass test\n",
    "    loss_value = mse_loss.forward(y_dummy, t_dummy)\n",
    "    assert isinstance(loss_value, float), f'Expected loss to be a float, but got {type(loss_value)}'\n",
    "\n",
    "    grad_input = mse_loss.backward()\n",
    "    assert grad_input.shape == (num_samples, output_dim), f'Expected grad_input shape: {(num_samples, output_dim)}, but got: {grad_input.shape}'\n",
    "\n",
    "    print('Visible shape test for MSELoss passed successfully!')\n",
    "\n",
    "test_mse_loss_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd5e3c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc403455db86c6c8107b188d10b1808d",
     "grade": true,
     "grade_id": "cell-5b93b89473b8ffa5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell contains hidden test cases that will be evaluated after submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9cb43",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42e6b354e2e329e78235d425f01584b0",
     "grade": false,
     "grade_id": "cell-47484bbeddf57913",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 5. Training loop\n",
    "\n",
    "In this section, we will visualize the training process of the MLP model on a simple synthetic dataset. The training loop will use the components you have implemented. No further implementation is required in this section. If everything is implemented correctly, you should see the model's predictions fit the data over iterations and your loss value should be less than 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428decfd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70079bd06d0323970ee3d329f2a90e08",
     "grade": false,
     "grade_id": "cell-d338462aa3898881",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "def generate_data(num_samples=100):\n",
    "    np.random.seed(4)\n",
    "    x = np.linspace(-1, 1, num_samples)\n",
    "    noise = np.random.randn(x.shape[0]) * 0.2\n",
    "    y = 2 * x**2 + 3 + noise\n",
    "    \n",
    "    x = np.expand_dims(x,1)\n",
    "    y = np.expand_dims(y,1)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3de4f-8950-4b14-9775-b02c255c30f6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c86c3888a6da34cc7ffe09c1810b6e63",
     "grade": false,
     "grade_id": "cell-92306aedbbb7642a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "update_plot = True # This will be used to visualize your training loss curve over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a451f44a-2ede-445b-a1af-81f3bb903bd4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e60745d512c9c422da0ad9c63a4bb60",
     "grade": true,
     "grade_id": "cell-19261f9b73886b5a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a69ea3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc1f184b71b70709e740c805498805f3",
     "grade": false,
     "grade_id": "cell-b9f088af4a1038ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 150\n",
    "# Initialize the MLP model and MSE Loss function\n",
    "mlp = MLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "mse_loss = MSELoss()\n",
    "\n",
    "x, y_train = generate_data()\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y_train, 'b.', label='Data points')  # Plot the original data\n",
    "line1, = ax.plot(x, np.full_like(y_train, min(y_train) - 0.1), 'r-', label='Predictions')\n",
    "ax.grid(True)\n",
    "ax.set_title(f'MLP Training - Iteration 0/{epochs}')\n",
    "ax.set_xlabel('Input')\n",
    "ax.set_ylabel('Output')\n",
    "plt.legend()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = mlp.forward(x)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = mse_loss.forward(y_pred, y_train)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    grad_loss = mse_loss.backward()\n",
    "    mlp.backward(grad_loss)\n",
    "    mlp.update_params(learning_rate)\n",
    "\n",
    "    if update_plot:\n",
    "        # Update the plot \n",
    "        line1.set_ydata(y_pred) \n",
    "        ax.set_title(f'MLP Training - Iteration {epoch + 1}/{epochs} - Loss: {loss:.4f}')\n",
    "        plt.pause(0.05)  \n",
    "        display.clear_output(wait=True)\n",
    "        display.display(fig)\n",
    "        \n",
    "display.clear_output(wait=True)\n",
    "plt.show()\n",
    "assert loss.item() < 0.15, 'Loss is too high, check your implementation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc62d41-e2d3-4c04-96ea-17dcf0907cb0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "765eea8b1e1fc58e8d0514a779f38293",
     "grade": true,
     "grade_id": "cell-5f6b7f9ae97ee862",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_course",
   "language": "python",
   "name": "dl_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
