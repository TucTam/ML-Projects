{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67631a04-b915-41b9-abda-c834efb90808",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a69a1f47705df784faccf9e3dfd99dbb",
     "grade": false,
     "grade_id": "cell-1521c2f2bb8e767a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Important! Please do not remove any cells, including the test cells, even if they appear empty. They contain hidden tests, and deleting them could result in a loss of points, as the exercises are graded automatically. Only edit the cells where you are instructed to write your solution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063830c3-2b10-444c-ad64-dc25f8db06c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d22d34fd978c53eeff6d91a7a33c59af",
     "grade": false,
     "grade_id": "cell-fe08d12516221b96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Exercise 1. Introduction to Gradient Calculations and PyTorch\n",
    "In this exercise, the goal is to understand the basics of building a neural network and training. The exercise is divided into three stages:\n",
    "\n",
    "**Part 1. Pen and Paper: Manual Gradient Calculation** (6 points): \n",
    "You will manually calculate the forward pass, loss computation, backward pass, and parameter updates for a simple neural network. This part will be completed in this notebook `ex1_01_pen_and_paper.ipynb`.\n",
    "\n",
    "**Part 2. NumPy Implementation for Network Training** (10 points): \n",
    "You will implement the layers used in a simple multi-layer perceptron on NumPy. This part will be completed in the notebook `ex1_02_numpy_gradient.ipynb`.\n",
    "\n",
    "**Part 3: PyTorch for Regression** (4 points):  You will build a neural network in PyTorch for a toy regression problem. This will introduce you to using PyTorch for model training. This part will be completed in the notebook `ex1_03_pytorch_regression.ipynb`.\n",
    "\n",
    "**Deliverables:** \\\n",
    "Submit the completed notebooks (_ex1_01_pen_and_paper.ipynb_, _ex1_02_numpy_gradient.ipynb_, _ex1_03_pytorch_regression.ipynb_) in separate files (no zip). Do not change the name of the notebook files as it may result in 0 points for the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d95a39-3037-4609-a529-84d5c4f56705",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f19a9c919fb118d005475bb120b5937",
     "grade": false,
     "grade_id": "cell-e55d6470fd0a90b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Quick Overview: Gradient Descent\n",
    "\n",
    "This section will review how forward and backward passes work and how parameters are updated during training.\n",
    "\n",
    "### 1. Forward pass\n",
    "\n",
    "The forward pass involves computing the output of each layer from its inputs by traversing through each block of the neural network from the first to the last layer.\n",
    "\n",
    "For each fully connected (linear) layer, the output is calculated using its input vector, weights, and biases as follows:\n",
    "\n",
    "$$\n",
    "a^{(l)} = W^{(l)}z^{(l-1)} + b^{(l)}. \n",
    "$$\n",
    "\n",
    "Here, $W^{(l)}$ and $b^{(l)}$ are the weights and the biases of the $l^{th}$ layer, respectively, while $z^{(l-1)}$ is the output from the previous layer.  For the first layer, $z^{(0)}$ represents the input to the network, commonly denoted as $x$. The output of the linear layer, $a^{(l)}$, is often referred to as the pre-activation. Next, we  apply a nonlinearity to compute the post-activation output:\n",
    "\n",
    "$$\n",
    "z^{(l)} = f(a^{(l)}),\n",
    "$$\n",
    "where $f$ represents the activation function (such as tanh, ReLU, etc.).\n",
    "\n",
    "Once the output layer produces a prediction $y$, the loss function is computed. In this case, we are using the mean squared error (MSE) loss:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^N (y_i - t_i)^2,\n",
    "$$\n",
    "\n",
    "where $y_i$ is the predicted value, $t_i$ is the actual target value, and $N$ is the number of data points.\n",
    "\n",
    "\n",
    "### 2. Backward pass\n",
    "\n",
    "The backward pass involves computing the gradients of the loss with respect to the model parameters (weights and biases) of each layer by traversing from the last layer to the first layer. This process uses the chain rule to propagate the error gradients backward through the network and is also known as **backpropagation**. During the backward pass, we compute the gradients for both the pre-activation and the post-activation outputs.\n",
    "\n",
    "\n",
    "### 3. Update parameters\n",
    "\n",
    "Once the gradients are calculated, the model parameters are updated using the gradient descent algorithm. The parameters are adjusted in the direction that reduces the loss function. This update step is performed using the following rule:\n",
    "\n",
    "$$\n",
    "\\theta^{(\\tau)} = \\theta^{(\\tau-1)} - \\eta \\cdot \\nabla_{\\theta}L^{(\\tau-1)},\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\theta$ represents the parameters (weights and biases), $\\eta$ is the learning rate, $\\tau$ labels the iteration step, and $\\nabla_{\\theta}L$ is the gradient of the loss function with respect to the parameters.\n",
    "\n",
    "In practice, training a neural network involves multiple iterations over the dataset. Each iteration is called an epoch. During each epoch, the dataset is typically divided into smaller subsets called batches. The model parameters are updated after each batch. This approach, known as mini-batch gradient descent, helps in efficient computation and faster convergence. However, for this assignment, we will perform the computation for only one iteration (one forward pass, one backward pass, and parameter update).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaf231b-d701-4786-9e8a-6d60f9c8096c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf99173f5714479b6f196bc8eeac7cc7",
     "grade": false,
     "grade_id": "cell-0deca2e9b66c5980",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Part 1. Pen and Paper - Manual Gradient Computation for a Simple MLP\n",
    "You are given the initial parameters and the model architecture below. You need to manually compute the steps for one full iteration of the model training and write the final output asked in each step to the given notebook cells. \n",
    "\n",
    "#### Model Architecture\n",
    "\n",
    "In this part of the assignment we will use a simple Multi-Layer Perceptron (MLP) with the following architecture:\n",
    "- **Input dimension**: 3\n",
    "- **One hidden layer** with 2 neurons and tanh activation\n",
    "- **Output layer** with 1 neuron \n",
    "- **Loss Function**: Mean Squared Error (MSE)\n",
    "\n",
    "\n",
    "#### Initial Parameters\n",
    "\n",
    "- Weights ($W_1$) and biases ($b_1$) of the hidden layer: \\\n",
    "$W_1 = \\begin{bmatrix} 0.1 &  0.3 & -0.5 \\\\ -0.2 & 0.4 & 0.6 \\end{bmatrix}$,   $b_1 = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}$\n",
    "\n",
    "- Weights ($W_2$) and biases ($b_2$) of the output layer: \\\n",
    "$W_2 = \\begin{bmatrix} 0.7 & -0.8 \\end{bmatrix}$,   $b_2 = \\begin{bmatrix} 0.3 \\end{bmatrix}$ \n",
    "\n",
    "#### Input and Target\n",
    "- Input: \\\n",
    "$ x = \\begin{bmatrix} 0.3 \\\\ 0.4 \\\\ -0.1 \\end{bmatrix} $\n",
    "\n",
    "- Target output: \\\n",
    "$t = \\begin{bmatrix} 0.5 \\end{bmatrix}$\n",
    "\n",
    "The forward and backward passes of the expected architecture are shown in Figure 1. Each gradient is computed step by step. Starting from the loss, you need to work through each layer by applying **chain rule** and propagating the errors backward through the network, such as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z}.\n",
    "$$\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <figure>\n",
    "        <img src=\"ex1_graph_01-3.png\" alt=\"Image Title\" style=\"width:900px; height:auto;\"/>\n",
    "        <figcaption>Figure 1: Forward and Backward Pass in the MLP</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "#### Steps to follow:\n",
    "1. **Forward pass**: Calculate the outputs for each layer.\n",
    "3. **Loss Calculation**: Calculate the MSE Loss.\n",
    "4. **Backward pass**: Calculate the gradients of the loss with respect to the input of the layers and activation functions as well as the layer's parameters.\n",
    "5. **Parameter update**: Update the parameters for the next iteration.\n",
    "\n",
    "\n",
    "#### Important Notes:\n",
    "1. Make sure your answers are accurate to **2 decimal places**. \n",
    "2. Pay attention to the matrix shapes. The layer inputs are provided as column vectors with the shape ($D_{in}$,), where $D_{in}$ is the input dimension. The weights are shaped as ($D_{out}$, $D_{in}$), where $D_{out}$ is the number of neurons in the next layer. The provided matrices are already in the correct shape with placeholders such as `a = [None, None]`. You are expected to replace the `None` value with the correct value. Do not change the given shapes.\n",
    "3. We will calculate the expected values in each step based on your previous answers. If you make a mistake in an intermediate step, you will not be penalized again in the next steps if your solution based on that mistake is still consistent. However, if a necessary intermediate result is missing, the following steps that depend on it will also be affected.\n",
    "4. **Do not forget** to remove `raise NotImplementedError()` sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60210d0-29cb-4da0-ae0a-f4801d0e8c35",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abdac775f3d746e5a6c392e81ea44889",
     "grade": false,
     "grade_id": "cell-4249d8215a9ffa33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step-by-Step Manual Calculation\n",
    "#### Step 1: Forward Pass\n",
    "\n",
    "##### 1. Hidden Layer Pre-Activation ($ a $): \n",
    "\n",
    "Compute $a =  W_1  x + b_1$ by plugging in the values:\n",
    "\n",
    "$\n",
    "a =  \\begin{bmatrix} 0.1 & 0.3 & -0.5 \\\\-0.2  & 0.4  & 0.6 \\end{bmatrix}  \\begin{bmatrix} 0.3 \\\\ 0.4 \\\\ -0.1 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix} = \\begin{bmatrix} a_{1,1} \\\\ a_{2,1} \\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "628fce54-300b-45a4-8783-8c71f812240f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b209220200db8f4ad22f610d6174058",
     "grade": false,
     "grade_id": "cell-9932206e36576cea",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden layer pre-activation (a)\n",
    "a = [None, None]\n",
    "x = [[0.1, 0.3, -0.5], [-0.2, 0.4, 0.6]]\n",
    "w1 = [0.3, 0.4, -0.1]\n",
    "b1 = [0.1, 0.2]\n",
    "\n",
    "def forward_pass(x, w1, b1, a):\n",
    "    for i in range(0,len(a)):\n",
    "        temp = 0\n",
    "        for j in range(len(w1)):\n",
    "            temp += x[i][j] * w1[j] \n",
    "            \n",
    "        a[i] = temp + b1[i]\n",
    "    return a\n",
    "\n",
    "a = forward_pass(x, w1, b1, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb96f4f7-0a0b-4f60-95b6-5b932e4cf5b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cf8593756236d751a25e23cd0c1b728",
     "grade": true,
     "grade_id": "cell-2361fef1075a367f",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell checks the hidden layer pre-activation. DO NOT DELETE THE CELL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3ff5e-090d-48e3-bb6a-5e15709c71de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d04f701a142a7d16f3e416b327d6cff7",
     "grade": false,
     "grade_id": "cell-0a638a33b23570d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### 2. Apply tanh Activation (to $ a $):\n",
    "\n",
    "We apply the tanh function element-wise: \n",
    "$$\n",
    "z = \\text{tanh}(a) = \\begin{bmatrix} \\text{tanh}(a_{1,1}) \\\\ \\text{tanh}(a_{2,1}) \\end{bmatrix} = \\begin{bmatrix} z_{1,1} \\\\ z_{2,1} \\end{bmatrix}, \n",
    "$$\n",
    "where the tanh function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{tanh}(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "54a28c50-4f01-468a-9a2d-5baef51c5652",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "095b99e9ea09005dd86c38a04267ba18",
     "grade": false,
     "grade_id": "cell-f3f9e0266b0f09f4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden layer activation (z)\n",
    "z = [None, None]\n",
    "e = 2.71828\n",
    "\n",
    "def activation_tanh(a, z):\n",
    "    for i in range(0,len(a)):\n",
    "        z[i] = (e**(a[i]-e**(-a[i])))/(e**a[i]+e**(-a[i]))\n",
    "    return z\n",
    "\n",
    "z = activation_tanh(a, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d57fba5a-d755-467a-8978-cf2d42ff6b26",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c739237ecefa5549ee66eb3aca12d11",
     "grade": true,
     "grade_id": "cell-eaff72df29d416f2",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell checks the tanh activation. DO NOT DELETE THE CELL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100be57f-526e-4152-953d-552e4ebc28ec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7df655b823f42faf2a1a935503371c16",
     "grade": false,
     "grade_id": "cell-daa33b9930003366",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### 3. Output Layer ($ y $):\n",
    "\n",
    "Compute $y = W_2  z + b_2$ by plugging in the values:\n",
    "\n",
    "$\n",
    "y =  \\begin{bmatrix} 0.7 & -0.8 \\end{bmatrix}   \\begin{bmatrix} z_{1,1} \\\\ z_{2,1} \\end{bmatrix}+ [0.3]\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dbe847e3-b41f-494d-a947-a8d623bfa2be",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7494424fbd1de0a65a856cd9dd8556c4",
     "grade": false,
     "grade_id": "cell-295846e2f8e1c8aa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Output layer (y)\n",
    "y = [None]\n",
    "w2 = [0.7, -0.8]\n",
    "b2 = [0.3]\n",
    "def output_pass(z, w, b, y):\n",
    "    for i in range(0,len(y)):\n",
    "        temp = 0\n",
    "        for j in range(len(w)):\n",
    "            temp += z[j] * w[j] \n",
    "            \n",
    "        y[i] = temp + b[i]\n",
    "    return y\n",
    "\n",
    "y = output_pass(z, w2, b2, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5caaf4de-6756-4f0b-963b-16f4aaeccd92",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e0333df910439c2189156730c214853",
     "grade": true,
     "grade_id": "cell-ed77f30a2a00d9de",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell checks the output layer. DO NOT DELETE THE CELL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d227f-a3ce-428e-a3bc-513028ac262c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "337df389ab6649c783965b545f4bdb88",
     "grade": false,
     "grade_id": "cell-b949356a032a72be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Step 2: Calculate the Loss\n",
    "\n",
    "Use the Mean Squared Error (MSE) formula to compute the loss for a single data point as $L = (y_1 - t_1)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "512a9a23-cd11-4f1c-ad4d-5513663dccce",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "937dae1112d9c5cdafbd5b0aee60dd2f",
     "grade": false,
     "grade_id": "cell-8a44ffb5c5085bc4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043926792710571816"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss calculation (MSE)\n",
    "loss = None\n",
    "t = [0.5]\n",
    "def MSELoss(y_pred, y_true):\n",
    "    loss = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        loss += (y_pred[i] - y_true[i])**2\n",
    "    return loss\n",
    "\n",
    "loss = MSELoss(y, t)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e22f9aca-170e-4fe6-a07b-89bcfa865928",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2f1127ce513c124f1399d83293a189e",
     "grade": true,
     "grade_id": "cell-8414f67fc820c446",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell checks the loss calculation. DO NOT DELETE THE CELL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ea5e3-3186-4542-a6dd-91409bc8b9b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94843dab5eb8b8153c89f88e917910b7",
     "grade": false,
     "grade_id": "cell-8e3c826e94a0dc90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Step 3: Backward Pass\n",
    "\n",
    "##### 1. Gradient of the Loss w.r.t the Output:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y} = \\frac{ \\partial}{\\partial y}(y - t)^2 = 2 \\times (y - t)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "559ab1d9-663d-4c71-a691-234c2ec3d596",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "154c502cd6d187fb1bfd3d75eccb19bd",
     "grade": false,
     "grade_id": "cell-c07586d73a64893c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Gradient of the loss wrt the output (dL/dy)\n",
    "dL_dy = [None]\n",
    "\n",
    "def gradient_loss(y_pred, y_true, dL_dy):\n",
    "    for i in range(len(y_pred)):\n",
    "        dL_dy[i] = 2*(y_pred[i] - y_true[i])\n",
    "    return dL_dy\n",
    "\n",
    "dL_dy = gradient_loss(y, t, dL_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c33891a2-4583-4586-95ea-ddac879f3818",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ba9452b152fac717dea17ca09a4e811",
     "grade": true,
     "grade_id": "cell-a5b85bb5aaaaab69",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell checks the gradient of the loss wrt output. DO NOT DELETE THE CELL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8dedf1-68d4-43c8-8392-81e9cd73555c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f0cadf6a00e1fcbe6f5afe112097735",
     "grade": false,
     "grade_id": "cell-c8c1388cba71f57b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### 2. Gradient of the Loss w.r.t the Output Layer Weights:\n",
    "Apply the chain rule:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_2} =  \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial W_2} = \\frac{\\partial L}{\\partial y} z^T \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f008a2b5-0fbd-4520-bca3-092b93cc4b69",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "398dfcce343ee9156e84f0c9d2171601",
     "grade": false,
     "grade_id": "cell-e2be66d35f43eed3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Gradient of the loss wrt output layer weights (dL/dW2)\n",
    "dL_dW2 = [[None, None]]\n",
    "\n",
    "def gradient_weights(dL_dy, z, dL_dW2):\n",
    "    for i in range(len(dL_dy)):\n",
    "        for j in range(len(z)):\n",
    "            dL_dW2[i][j] = dL_dy[i] * z[j]\n",
    "    return dL_dW2\n",
    "\n",
    "dL_dW2 = gradient_weights(dL_dy, z, dL_dW2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8bba5bf7-d203-4846-a65b-472aaa64005a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89ffd8e8522448df81ea25ee99195807",
     "grade": true,
     "grade_id": "cell-84eef4f7666cb862",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell checks the gradient of the loss wrt output layer weights. DO NOT DELETE THE CELL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf5fb1-c802-4673-a8df-a31c1ef37bc7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8664176f59c9558786c552ac3784ee25",
     "grade": false,
     "grade_id": "cell-91d2f7bb78125968",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### 3. Gradient of the Loss w.r.t the Output Layer Bias:\n",
    "\n",
    "Chain rule applies; compute  $\n",
    "\\frac{\\partial \\text{L}}{\\partial b_2} \n",
    "$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "400a2f6a-b3fa-46ec-8330-07c17466f5a6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "615e2b4b3488e8945413e5387512a9ed",
     "grade": false,
     "grade_id": "cell-8e42ebdfc6ac608c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Gradient of the loss wrt output layer bias (dL/db2)\n",
    "dL_db2 = [None]\n",
    "\n",
    "def gradient_bias(dL_dy, dL_db2):\n",
    "    for i in range(len(dL_dy)):\n",
    "        dL_db2[i] = dL_dy[i]\n",
    "    return dL_db2\n",
    "\n",
    "dL_db2 = gradient_bias(dL_dy, dL_db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "374a6086-096d-4e69-b0da-8ac5ad3587d0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f12c193778f736827806f17417a23e07",
     "grade": true,
     "grade_id": "cell-91766dd565d38716",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell checks the gradient of the loss wrt output layer bias. DO NOT DELETE THE CELL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be094834-b4a0-4b29-8dc0-a6806a831403",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c0f9a2caba94afb40e4826caa07f40b",
     "grade": false,
     "grade_id": "cell-b89384c06f8d5333",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### 4. Gradient of the Loss w.r.t the Hidden Layer Weights: \n",
    "To compute $\\frac{\\partial \\text{L}}{\\partial W_1} $, you will need to apply the chain rule by combining the relevant partial derivatives. \n",
    "\n",
    "##### Hints: \n",
    "1. You need to compute four partial derivatives in this step, one of which you have already obtained.\n",
    "2. Do not forget to compute the derivative of the activation function **tanh**.\n",
    "3. Refer to the given computational graph to ensure that you are working through each layer to compute the full gradient. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8edbff1f-cdab-474e-ba59-5bb5105cb433",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa353aba59e2758c7a352421cc8b28d4",
     "grade": false,
     "grade_id": "cell-b8147e2233bbd6f0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Gradient of the loss wrt hidden layer weights (dL/dW1)\n",
    "dL_dW1 = [[None, None, None],\n",
    "         [None,  None, None]]\n",
    "\n",
    "def gradient_hidden_weights(dL_dy, w2, z, x, dL_dW1):\n",
    "    for i in range(len(dL_dW1)):\n",
    "        for j in range(len(dL_dW1[0])):\n",
    "            dL_dW1[i][j] = dL_dy[0] * w2[i] * (1 - z[i]**2) * x[i][j]\n",
    "    return dL_dW1\n",
    "\n",
    "dL_dW1 = gradient_hidden_weights(dL_dy, w2, z, x, dL_dW1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "774371a5-2606-4e6f-b13b-b3abab7f1edd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf0ac45956e8955471ae6ea2cd89cede",
     "grade": true,
     "grade_id": "cell-3e108c28c05c3378",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell checks the gradient of the loss wrt hidden layer weights. DO NOT DELETE THE CELL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382076b1-e2fb-4e1b-81fb-2932eb65eff2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fba24312dbc0cdf3f7e1a4d345e0ac97",
     "grade": false,
     "grade_id": "cell-1eeff67a0ee05a94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Step 4: Parameter Update\n",
    "\n",
    "Using a learning rate of $\\eta = 0.5$, update the parameters:\n",
    "\n",
    "##### 1. Update the Output Layer Weights:\n",
    "\n",
    "$\n",
    "W_2 = W_2 - \\eta \\times \\frac{\\partial \\text{L}}{\\partial W_2}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ad319bca-1a36-4c3e-a32f-199cb30f53ef",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1fe3251b3e4138782c74b8e3101c3c4",
     "grade": false,
     "grade_id": "cell-482444f2a7aff90f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Updated output layer weights (W2)\n",
    "W2_updated = [[None, None]]\n",
    "learning_rate = 0.5\n",
    "\n",
    "def update_output_weights(W, dL_dW, learning_rate):\n",
    "    for i in range(len(W)):\n",
    "        W[i] = W[i] - learning_rate * dL_dW[0][i]\n",
    "    return W\n",
    "\n",
    "W2_updated = update_output_weights(w2, dL_dW2, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "61216d71-6b3f-4a1f-8ddb-40fab82e390f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "217fd3bc430a6f2350a299421dafc888",
     "grade": true,
     "grade_id": "cell-f333a26e47e6d03a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell checks the updated output layer weights. DO NOT DELETE THE CELL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a3ed5b-e6ed-4789-b6b9-52ecdaf5ea1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96670d06f34f3ce955dd87767e2dc8bf",
     "grade": false,
     "grade_id": "cell-de7decc3834641b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### 2. Update the Output Layer Bias:\n",
    "\n",
    "$\n",
    "b_2 = b_2 - \\eta \\times \\frac{\\partial \\text{L}}{\\partial b_2}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e77c056c-bc9c-4843-b913-e664bec5c4b5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8d21e3b892e4895db9acdac85622ddc",
     "grade": false,
     "grade_id": "cell-d6f8084054d83788",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Updated output layer bias (b2)\n",
    "b2_updated = [None]\n",
    "\n",
    "def update_output_bias(b, dL_db, learning_rate):\n",
    "    for i in range(len(b)):\n",
    "        b[i] = b[i] - learning_rate * dL_db[i]\n",
    "    return b\n",
    "\n",
    "b2_updated = update_output_bias(b2, dL_db2, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "28e0ba78-51e4-431c-a27a-d83a5939ce59",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9de42d5bbe693a72071e548be6d235ee",
     "grade": true,
     "grade_id": "cell-b2e2a9d7f6a13395",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell checks the updated output layer bias. DO NOT DELETE THE CELL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9b358-d275-4ae4-ae48-9c1d2a1737b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5afc01c4288edbd510dcd9ce300a1d32",
     "grade": false,
     "grade_id": "cell-0456c37e23e5cfad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### 3. Update the Hidden Layer Weights:\n",
    "\n",
    "$\n",
    "W_1 = W_1 - \\eta \\times \\frac{\\partial \\text{L}}{\\partial W_1}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "48f311a7-0fdd-436d-8b0a-d7450c28c86c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69043603349abf554e38e1fb10b467f2",
     "grade": false,
     "grade_id": "cell-b336179e9e02339f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Updated hidden layer weights (W1)\n",
    "W1_updated = [[None, None, None],\n",
    "             [None, None, None]]\n",
    "\n",
    "def update_hidden_weights(W, dL_dW, learning_rate):\n",
    "    for i in range(len(W)):\n",
    "        W[i] = W[i] - learning_rate * dL_dW[0][i]\n",
    "    return W\n",
    "\n",
    "W1_updated = update_hidden_weights(w1, dL_dW1, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b6b68589-e7f4-4b6b-aedc-d5735f47a4e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f1664c2c43a095068f4de777a5dbc83",
     "grade": true,
     "grade_id": "cell-f762367b8ae24d0f",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell checks the updated hidden layer weights. DO NOT DELETE THE CELL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae492a08-d325-42f4-938e-3543d0150197",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd11ba19799dcbc88626f94c39a9f018",
     "grade": false,
     "grade_id": "cell-0368f3a0493bebbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### 4. Update the Hidden Layer Biases:\n",
    "\n",
    "$\n",
    "b_1 = b_1 - \\eta \\times \\frac{\\partial \\text{L}}{\\partial b_1}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4189891a-55a2-4eb6-b64f-0fee2eb7175f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96fdfe89a7ca2e1ddfad5ead89b23c6d",
     "grade": false,
     "grade_id": "cell-dea5c55012f1d2eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Updated hidden layer bias (b1\n",
    "b1_updated = [None, None]\n",
    "dl_db1 = [None, None]\n",
    "\n",
    "def gradient_hidden_bias(dL_dy, w2, z, b1, dL_db1):\n",
    "    for i in range(len(dL_db1)):\n",
    "        dL_db1[i] = dL_dy[0] * w2[i] * (1 - z[i]**2)\n",
    "    return dL_db1\n",
    "\n",
    "dl_db1 = gradient_hidden_bias(dL_dy, w2, z, b1, dl_db1)\n",
    "\n",
    "def update_hidden_bias(b, dL_db, learning_rate):\n",
    "    for i in range(len(b)):\n",
    "        b[i] = b[i] - learning_rate * dL_db[i]\n",
    "    return b\n",
    "\n",
    "b1_updated = update_hidden_bias(b1, dl_db1, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7de1f658-f334-43c9-a541-77b5d88fbd80",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da905305961ac8c35ab2ffae996e0719",
     "grade": true,
     "grade_id": "cell-bd45706e3f06a20d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell checks the updated hidden layer bias. DO NOT DELETE THE CELL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "88e682c0-591e-4c0d-b7f5-47876b5e02d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "322d6f8f52263751ea1b0278453b49e0",
     "grade": true,
     "grade_id": "cell-87198f1f64968875",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
