{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8352d0-eb1e-4e35-af17-2382477d5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "db = pd.read_csv('../datasets/top10stock.csv')\n",
    "\n",
    "select = db.iloc[:, 0] == \"AAPL\"\n",
    "apple = db.loc[select, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcf6960-d29f-4287-a58b-9319c1087ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformatting data, datapreprocessing\n",
    "for row in range(2516):\n",
    "    for col in range(8):\n",
    "        if col in [2, 4, 5, 6]:\n",
    "            apple.iloc[row,col] = apple.iloc[row,col].strip(\"$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cac76a1-85fa-4643-a47b-cb4dff971381",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_objects = []\n",
    "for strp in apple.Date:\n",
    "    obj = None\n",
    "    if strp[2] == \"/\":\n",
    "        obj = datetime.strptime(strp, \"%m/%d/%Y\")\n",
    "    else:\n",
    "        obj = datetime.strptime(strp, \"%m-%d-%Y\")\n",
    "    date_objects.append(obj)\n",
    "days = pd.DataFrame([date_objects[i].day for i in range(len(date_objects))])\n",
    "months = pd.DataFrame([date_objects[i].month for i in range(len(date_objects))])\n",
    "years = pd.DataFrame([date_objects[i].year for i in range(len(date_objects))])\n",
    "days.columns = [\"Day\"]\n",
    "months.columns = [\"Month\"]\n",
    "years.columns = [\"Year\"]\n",
    "\n",
    "# splitting variables into its features and labels\n",
    "features = pd.concat([days, months, years, apple.iloc[:, [3,4,5,6]]], axis=1)\n",
    "labels = pd.DataFrame(apple.iloc[:,2])\n",
    "\n",
    "# Converting their datatypes\n",
    "features = features.astype('float32')\n",
    "labels = labels.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fdeefb8-125a-4047-977e-8e36cccf60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hyperparameters\n",
    "epochs = 4\n",
    "batchsize = 2\n",
    "lr = 0.0001\n",
    "inputsize = 7\n",
    "hid_layer1_sz = 16\n",
    "hid_layer2_sz = 16\n",
    "hid_layer3_sz = 16\n",
    "class_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4b9783-fa9d-4742-865e-3a8e4ed9a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling our data\n",
    "ftmean = np.nanmean(features)\n",
    "ftstd = features.values.std()\n",
    "lbmean = np.nanmean(labels)\n",
    "lbstd = labels.std()\n",
    "\n",
    "features = pd.DataFrame((features.values - ftmean) / ftstd)\n",
    "labels = (labels - lbmean) / lbstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ff44b-3ff3-4c01-8374-969a6ede2d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "203dd603-458f-4401-a7e2-33c5a75255bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a database class that the dataloader can use. (Dataloader is the what we will be using to train and test)\n",
    "class database(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features.iloc[idx].values\n",
    "        label = self.labels.iloc[idx].values\n",
    "        feature = torch.tensor(feature)\n",
    "        label = torch.tensor(label)\n",
    "        return feature, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2611de80-f8e3-4cba-a620-bf3e8a81dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training data and testing data\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(features,labels,train_size=0.8)\n",
    "\n",
    "# initializing our database class with the splitted data\n",
    "train_db = database(xtrain,ytrain)\n",
    "test_db = database(xtest,ytest)\n",
    "\n",
    "# Creating our dataloader and transforming it into batches, etc.\n",
    "trainloader = torch.utils.data.DataLoader(train_db, batch_size=batchsize, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(test_db, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "228cca52-b0b8-4ce6-98ee-b8ecf9426d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2012/503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b25d4ecd-e839-4937-af0c-075f12f0e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating our model architechture\n",
    "class stock_classification(nn.Module):\n",
    "    def __init__(self, inputsize, hidden1, hidden2, hidden3, class_sz):\n",
    "        super(stock_classification, self).__init__()\n",
    "        self.l1 = nn.Linear(inputsize,hidden1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden1, hidden2)\n",
    "        self.l3 = nn.Linear(hidden2, hidden3)\n",
    "        self.l4 = nn.Linear(hidden3, class_sz)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.l4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "234ed8a7-5502-43ee-af1c-bcdf4c60d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing our model, loss function, and optimizer\n",
    "model = stock_classification(inputsize, hid_layer1_sz,hid_layer2_sz,hid_layer3_sz,class_size).to(device)\n",
    "model.train()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3799a270-e372-4b75-80f3-da622ba4421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [4/1006], loss: 1.8806\n",
      "Epoch [1/4], Step [8/1006], loss: 0.7162\n",
      "Epoch [1/4], Step [12/1006], loss: 0.5354\n",
      "Epoch [1/4], Step [16/1006], loss: 0.1054\n",
      "Epoch [1/4], Step [20/1006], loss: 0.3456\n",
      "Epoch [1/4], Step [24/1006], loss: 3.2668\n",
      "Epoch [1/4], Step [28/1006], loss: 0.2845\n",
      "Epoch [1/4], Step [32/1006], loss: 0.5348\n",
      "Epoch [1/4], Step [36/1006], loss: 0.2199\n",
      "Epoch [1/4], Step [40/1006], loss: 1.9628\n",
      "Epoch [1/4], Step [44/1006], loss: 1.2648\n",
      "Epoch [1/4], Step [48/1006], loss: 1.9084\n",
      "Epoch [1/4], Step [52/1006], loss: 1.6385\n",
      "Epoch [1/4], Step [56/1006], loss: 1.9183\n",
      "Epoch [1/4], Step [60/1006], loss: 3.1255\n",
      "Epoch [1/4], Step [64/1006], loss: 1.3268\n",
      "Epoch [1/4], Step [68/1006], loss: 1.8466\n",
      "Epoch [1/4], Step [72/1006], loss: 0.2329\n",
      "Epoch [1/4], Step [76/1006], loss: 0.3404\n",
      "Epoch [1/4], Step [80/1006], loss: 0.3378\n",
      "Epoch [1/4], Step [84/1006], loss: 1.2260\n",
      "Epoch [1/4], Step [88/1006], loss: 0.3966\n",
      "Epoch [1/4], Step [92/1006], loss: 1.3459\n",
      "Epoch [1/4], Step [96/1006], loss: 0.6647\n",
      "Epoch [1/4], Step [100/1006], loss: 0.4099\n",
      "Epoch [1/4], Step [104/1006], loss: 0.8290\n",
      "Epoch [1/4], Step [108/1006], loss: 1.4564\n",
      "Epoch [1/4], Step [112/1006], loss: 1.2274\n",
      "Epoch [1/4], Step [116/1006], loss: 0.2553\n",
      "Epoch [1/4], Step [120/1006], loss: 0.5013\n",
      "Epoch [1/4], Step [124/1006], loss: 1.5649\n",
      "Epoch [1/4], Step [128/1006], loss: 0.5286\n",
      "Epoch [1/4], Step [132/1006], loss: 0.4305\n",
      "Epoch [1/4], Step [136/1006], loss: 0.3582\n",
      "Epoch [1/4], Step [140/1006], loss: 2.0969\n",
      "Epoch [1/4], Step [144/1006], loss: 1.3910\n",
      "Epoch [1/4], Step [148/1006], loss: 0.4581\n",
      "Epoch [1/4], Step [152/1006], loss: 2.6736\n",
      "Epoch [1/4], Step [156/1006], loss: 0.4149\n",
      "Epoch [1/4], Step [160/1006], loss: 0.2608\n",
      "Epoch [1/4], Step [164/1006], loss: 1.6342\n",
      "Epoch [1/4], Step [168/1006], loss: 0.2438\n",
      "Epoch [1/4], Step [172/1006], loss: 1.1935\n",
      "Epoch [1/4], Step [176/1006], loss: 2.1079\n",
      "Epoch [1/4], Step [180/1006], loss: 1.0680\n",
      "Epoch [1/4], Step [184/1006], loss: 1.1448\n",
      "Epoch [1/4], Step [188/1006], loss: 0.4947\n",
      "Epoch [1/4], Step [192/1006], loss: 2.5008\n",
      "Epoch [1/4], Step [196/1006], loss: 0.4048\n",
      "Epoch [1/4], Step [200/1006], loss: 2.3892\n",
      "Epoch [1/4], Step [204/1006], loss: 2.1471\n",
      "Epoch [1/4], Step [208/1006], loss: 1.4268\n",
      "Epoch [1/4], Step [212/1006], loss: 0.7766\n",
      "Epoch [1/4], Step [216/1006], loss: 0.2936\n",
      "Epoch [1/4], Step [220/1006], loss: 0.9489\n",
      "Epoch [1/4], Step [224/1006], loss: 0.8063\n",
      "Epoch [1/4], Step [228/1006], loss: 0.7231\n",
      "Epoch [1/4], Step [232/1006], loss: 0.5638\n",
      "Epoch [1/4], Step [236/1006], loss: 4.3573\n",
      "Epoch [1/4], Step [240/1006], loss: 0.3834\n",
      "Epoch [1/4], Step [244/1006], loss: 2.4459\n",
      "Epoch [1/4], Step [248/1006], loss: 0.9292\n",
      "Epoch [1/4], Step [252/1006], loss: 0.4505\n",
      "Epoch [1/4], Step [256/1006], loss: 0.8488\n",
      "Epoch [1/4], Step [260/1006], loss: 0.6198\n",
      "Epoch [1/4], Step [264/1006], loss: 2.9787\n",
      "Epoch [1/4], Step [268/1006], loss: 0.5669\n",
      "Epoch [1/4], Step [272/1006], loss: 0.0351\n",
      "Epoch [1/4], Step [276/1006], loss: 2.5896\n",
      "Epoch [1/4], Step [280/1006], loss: 2.0652\n",
      "Epoch [1/4], Step [284/1006], loss: 0.2248\n",
      "Epoch [1/4], Step [288/1006], loss: 1.1907\n",
      "Epoch [1/4], Step [292/1006], loss: 2.0093\n",
      "Epoch [1/4], Step [296/1006], loss: 0.3473\n",
      "Epoch [1/4], Step [300/1006], loss: 0.2777\n",
      "Epoch [1/4], Step [304/1006], loss: 1.4867\n",
      "Epoch [1/4], Step [308/1006], loss: 0.3310\n",
      "Epoch [1/4], Step [312/1006], loss: 2.0695\n",
      "Epoch [1/4], Step [316/1006], loss: 1.4320\n",
      "Epoch [1/4], Step [320/1006], loss: 2.0093\n",
      "Epoch [1/4], Step [324/1006], loss: 1.2448\n",
      "Epoch [1/4], Step [328/1006], loss: 0.4031\n",
      "Epoch [1/4], Step [332/1006], loss: 3.0489\n",
      "Epoch [1/4], Step [336/1006], loss: 0.5061\n",
      "Epoch [1/4], Step [340/1006], loss: 2.2528\n",
      "Epoch [1/4], Step [344/1006], loss: 1.2130\n",
      "Epoch [1/4], Step [348/1006], loss: 1.7593\n",
      "Epoch [1/4], Step [352/1006], loss: 1.8486\n",
      "Epoch [1/4], Step [356/1006], loss: 0.2072\n",
      "Epoch [1/4], Step [360/1006], loss: 1.8756\n",
      "Epoch [1/4], Step [364/1006], loss: 0.2513\n",
      "Epoch [1/4], Step [368/1006], loss: 0.3518\n",
      "Epoch [1/4], Step [372/1006], loss: 1.3199\n",
      "Epoch [1/4], Step [376/1006], loss: 0.5452\n",
      "Epoch [1/4], Step [380/1006], loss: 0.3545\n",
      "Epoch [1/4], Step [384/1006], loss: 0.6095\n",
      "Epoch [1/4], Step [388/1006], loss: 1.9717\n",
      "Epoch [1/4], Step [392/1006], loss: 0.9356\n",
      "Epoch [1/4], Step [396/1006], loss: 1.0985\n",
      "Epoch [1/4], Step [400/1006], loss: 1.9881\n",
      "Epoch [1/4], Step [404/1006], loss: 0.8817\n",
      "Epoch [1/4], Step [408/1006], loss: 2.5738\n",
      "Epoch [1/4], Step [412/1006], loss: 1.4921\n",
      "Epoch [1/4], Step [416/1006], loss: 0.4007\n",
      "Epoch [1/4], Step [420/1006], loss: 0.9405\n",
      "Epoch [1/4], Step [424/1006], loss: 0.3151\n",
      "Epoch [1/4], Step [428/1006], loss: 1.9826\n",
      "Epoch [1/4], Step [432/1006], loss: 0.2230\n",
      "Epoch [1/4], Step [436/1006], loss: 1.7620\n",
      "Epoch [1/4], Step [440/1006], loss: 0.1851\n",
      "Epoch [1/4], Step [444/1006], loss: 1.0742\n",
      "Epoch [1/4], Step [448/1006], loss: 1.2672\n",
      "Epoch [1/4], Step [452/1006], loss: 0.8196\n",
      "Epoch [1/4], Step [456/1006], loss: 1.2793\n",
      "Epoch [1/4], Step [460/1006], loss: 0.1791\n",
      "Epoch [1/4], Step [464/1006], loss: 0.4091\n",
      "Epoch [1/4], Step [468/1006], loss: 0.6678\n",
      "Epoch [1/4], Step [472/1006], loss: 0.4308\n",
      "Epoch [1/4], Step [476/1006], loss: 0.2238\n",
      "Epoch [1/4], Step [480/1006], loss: 0.9990\n",
      "Epoch [1/4], Step [484/1006], loss: 0.2700\n",
      "Epoch [1/4], Step [488/1006], loss: 0.4971\n",
      "Epoch [1/4], Step [492/1006], loss: 0.9029\n",
      "Epoch [1/4], Step [496/1006], loss: 2.4644\n",
      "Epoch [1/4], Step [500/1006], loss: 0.4185\n",
      "Epoch [1/4], Step [504/1006], loss: 0.6936\n",
      "Epoch [1/4], Step [508/1006], loss: 0.7448\n",
      "Epoch [1/4], Step [512/1006], loss: 0.7565\n",
      "Epoch [1/4], Step [516/1006], loss: 0.4653\n",
      "Epoch [1/4], Step [520/1006], loss: 0.3665\n",
      "Epoch [1/4], Step [524/1006], loss: 0.7182\n",
      "Epoch [1/4], Step [528/1006], loss: 0.5286\n",
      "Epoch [1/4], Step [532/1006], loss: 1.8947\n",
      "Epoch [1/4], Step [536/1006], loss: 1.9218\n",
      "Epoch [1/4], Step [540/1006], loss: 0.4790\n",
      "Epoch [1/4], Step [544/1006], loss: 1.1957\n",
      "Epoch [1/4], Step [548/1006], loss: 1.6181\n",
      "Epoch [1/4], Step [552/1006], loss: 0.2524\n",
      "Epoch [1/4], Step [556/1006], loss: 0.9580\n",
      "Epoch [1/4], Step [560/1006], loss: 0.0461\n",
      "Epoch [1/4], Step [564/1006], loss: 0.3243\n",
      "Epoch [1/4], Step [568/1006], loss: 0.2318\n",
      "Epoch [1/4], Step [572/1006], loss: 0.4719\n",
      "Epoch [1/4], Step [576/1006], loss: 1.7296\n",
      "Epoch [1/4], Step [580/1006], loss: 0.2775\n",
      "Epoch [1/4], Step [584/1006], loss: 1.4271\n",
      "Epoch [1/4], Step [588/1006], loss: 0.4088\n",
      "Epoch [1/4], Step [592/1006], loss: 0.2134\n",
      "Epoch [1/4], Step [596/1006], loss: 0.8497\n",
      "Epoch [1/4], Step [600/1006], loss: 0.3440\n",
      "Epoch [1/4], Step [604/1006], loss: 0.2293\n",
      "Epoch [1/4], Step [608/1006], loss: 0.3028\n",
      "Epoch [1/4], Step [612/1006], loss: 2.4946\n",
      "Epoch [1/4], Step [616/1006], loss: 1.5854\n",
      "Epoch [1/4], Step [620/1006], loss: 1.5550\n",
      "Epoch [1/4], Step [624/1006], loss: 0.2931\n",
      "Epoch [1/4], Step [628/1006], loss: 0.3060\n",
      "Epoch [1/4], Step [632/1006], loss: 0.3106\n",
      "Epoch [1/4], Step [636/1006], loss: 0.3617\n",
      "Epoch [1/4], Step [640/1006], loss: 0.8084\n",
      "Epoch [1/4], Step [644/1006], loss: 0.0583\n",
      "Epoch [1/4], Step [648/1006], loss: 0.2362\n",
      "Epoch [1/4], Step [652/1006], loss: 3.5408\n",
      "Epoch [1/4], Step [656/1006], loss: 0.3553\n",
      "Epoch [1/4], Step [660/1006], loss: 3.4717\n",
      "Epoch [1/4], Step [664/1006], loss: 1.0151\n",
      "Epoch [1/4], Step [668/1006], loss: 1.1780\n",
      "Epoch [1/4], Step [672/1006], loss: 0.4400\n",
      "Epoch [1/4], Step [676/1006], loss: 0.9726\n",
      "Epoch [1/4], Step [680/1006], loss: 0.4055\n",
      "Epoch [1/4], Step [684/1006], loss: 0.6297\n",
      "Epoch [1/4], Step [688/1006], loss: 0.3815\n",
      "Epoch [1/4], Step [692/1006], loss: 2.2434\n",
      "Epoch [1/4], Step [696/1006], loss: 0.4870\n",
      "Epoch [1/4], Step [700/1006], loss: 2.0110\n",
      "Epoch [1/4], Step [704/1006], loss: 0.4561\n",
      "Epoch [1/4], Step [708/1006], loss: 1.2001\n",
      "Epoch [1/4], Step [712/1006], loss: 1.9261\n",
      "Epoch [1/4], Step [716/1006], loss: 1.1774\n",
      "Epoch [1/4], Step [720/1006], loss: 0.9828\n",
      "Epoch [1/4], Step [724/1006], loss: 0.3095\n",
      "Epoch [1/4], Step [728/1006], loss: 0.3792\n",
      "Epoch [1/4], Step [732/1006], loss: 0.4996\n",
      "Epoch [1/4], Step [736/1006], loss: 0.1929\n",
      "Epoch [1/4], Step [740/1006], loss: 0.4143\n",
      "Epoch [1/4], Step [744/1006], loss: 1.2754\n",
      "Epoch [1/4], Step [748/1006], loss: 1.3388\n",
      "Epoch [1/4], Step [752/1006], loss: 0.2735\n",
      "Epoch [1/4], Step [756/1006], loss: 0.9559\n",
      "Epoch [1/4], Step [760/1006], loss: 2.4636\n",
      "Epoch [1/4], Step [764/1006], loss: 0.9327\n",
      "Epoch [1/4], Step [768/1006], loss: 0.6248\n",
      "Epoch [1/4], Step [772/1006], loss: 1.6531\n",
      "Epoch [1/4], Step [776/1006], loss: 0.4685\n",
      "Epoch [1/4], Step [780/1006], loss: 0.4970\n",
      "Epoch [1/4], Step [784/1006], loss: 0.3555\n",
      "Epoch [1/4], Step [788/1006], loss: 2.0914\n",
      "Epoch [1/4], Step [792/1006], loss: 0.5859\n",
      "Epoch [1/4], Step [796/1006], loss: 0.7388\n",
      "Epoch [1/4], Step [800/1006], loss: 0.1186\n",
      "Epoch [1/4], Step [804/1006], loss: 1.7816\n",
      "Epoch [1/4], Step [808/1006], loss: 1.3992\n",
      "Epoch [1/4], Step [812/1006], loss: 0.5010\n",
      "Epoch [1/4], Step [816/1006], loss: 0.3564\n",
      "Epoch [1/4], Step [820/1006], loss: 0.5915\n",
      "Epoch [1/4], Step [824/1006], loss: 1.6611\n",
      "Epoch [1/4], Step [828/1006], loss: 2.6195\n",
      "Epoch [1/4], Step [832/1006], loss: 0.4324\n",
      "Epoch [1/4], Step [836/1006], loss: 0.2631\n",
      "Epoch [1/4], Step [840/1006], loss: 0.9005\n",
      "Epoch [1/4], Step [844/1006], loss: 0.4981\n",
      "Epoch [1/4], Step [848/1006], loss: 1.1542\n",
      "Epoch [1/4], Step [852/1006], loss: 1.5001\n",
      "Epoch [1/4], Step [856/1006], loss: 0.1209\n",
      "Epoch [1/4], Step [860/1006], loss: 2.4340\n",
      "Epoch [1/4], Step [864/1006], loss: 0.1244\n",
      "Epoch [1/4], Step [868/1006], loss: 0.6360\n",
      "Epoch [1/4], Step [872/1006], loss: 0.4623\n",
      "Epoch [1/4], Step [876/1006], loss: 0.7029\n",
      "Epoch [1/4], Step [880/1006], loss: 1.0002\n",
      "Epoch [1/4], Step [884/1006], loss: 1.1606\n",
      "Epoch [1/4], Step [888/1006], loss: 0.3924\n",
      "Epoch [1/4], Step [892/1006], loss: 0.4574\n",
      "Epoch [1/4], Step [896/1006], loss: 0.2235\n",
      "Epoch [1/4], Step [900/1006], loss: 0.4406\n",
      "Epoch [1/4], Step [904/1006], loss: 1.6161\n",
      "Epoch [1/4], Step [908/1006], loss: 0.3142\n",
      "Epoch [1/4], Step [912/1006], loss: 0.2741\n",
      "Epoch [1/4], Step [916/1006], loss: 1.8272\n",
      "Epoch [1/4], Step [920/1006], loss: 1.1506\n",
      "Epoch [1/4], Step [924/1006], loss: 1.9169\n",
      "Epoch [1/4], Step [928/1006], loss: 0.9482\n",
      "Epoch [1/4], Step [932/1006], loss: 0.2894\n",
      "Epoch [1/4], Step [936/1006], loss: 0.5889\n",
      "Epoch [1/4], Step [940/1006], loss: 0.6567\n",
      "Epoch [1/4], Step [944/1006], loss: 0.6417\n",
      "Epoch [1/4], Step [948/1006], loss: 1.7599\n",
      "Epoch [1/4], Step [952/1006], loss: 0.9934\n",
      "Epoch [1/4], Step [956/1006], loss: 2.1648\n",
      "Epoch [1/4], Step [960/1006], loss: 1.6066\n",
      "Epoch [1/4], Step [964/1006], loss: 0.4125\n",
      "Epoch [1/4], Step [968/1006], loss: 0.3975\n",
      "Epoch [1/4], Step [972/1006], loss: 0.7089\n",
      "Epoch [1/4], Step [976/1006], loss: 0.8598\n",
      "Epoch [1/4], Step [980/1006], loss: 0.5776\n",
      "Epoch [1/4], Step [984/1006], loss: 0.5871\n",
      "Epoch [1/4], Step [988/1006], loss: 1.5169\n",
      "Epoch [1/4], Step [992/1006], loss: 2.3979\n",
      "Epoch [1/4], Step [996/1006], loss: 1.8994\n",
      "Epoch [1/4], Step [1000/1006], loss: 0.1075\n",
      "Epoch [1/4], Step [1004/1006], loss: 0.2840\n",
      "Epoch [2/4], Step [4/1006], loss: 1.1588\n",
      "Epoch [2/4], Step [8/1006], loss: 0.7220\n",
      "Epoch [2/4], Step [12/1006], loss: 0.6371\n",
      "Epoch [2/4], Step [16/1006], loss: 0.1966\n",
      "Epoch [2/4], Step [20/1006], loss: 0.5875\n",
      "Epoch [2/4], Step [24/1006], loss: 2.6433\n",
      "Epoch [2/4], Step [28/1006], loss: 0.4485\n",
      "Epoch [2/4], Step [32/1006], loss: 0.6340\n",
      "Epoch [2/4], Step [36/1006], loss: 0.1654\n",
      "Epoch [2/4], Step [40/1006], loss: 1.3742\n",
      "Epoch [2/4], Step [44/1006], loss: 0.8761\n",
      "Epoch [2/4], Step [48/1006], loss: 1.4896\n",
      "Epoch [2/4], Step [52/1006], loss: 1.1962\n",
      "Epoch [2/4], Step [56/1006], loss: 1.3075\n",
      "Epoch [2/4], Step [60/1006], loss: 2.4890\n",
      "Epoch [2/4], Step [64/1006], loss: 0.8049\n",
      "Epoch [2/4], Step [68/1006], loss: 1.2549\n",
      "Epoch [2/4], Step [72/1006], loss: 0.4711\n",
      "Epoch [2/4], Step [76/1006], loss: 0.5846\n",
      "Epoch [2/4], Step [80/1006], loss: 0.7269\n",
      "Epoch [2/4], Step [84/1006], loss: 0.7551\n",
      "Epoch [2/4], Step [88/1006], loss: 0.6303\n",
      "Epoch [2/4], Step [92/1006], loss: 0.9172\n",
      "Epoch [2/4], Step [96/1006], loss: 0.5531\n",
      "Epoch [2/4], Step [100/1006], loss: 0.4065\n",
      "Epoch [2/4], Step [104/1006], loss: 0.5370\n",
      "Epoch [2/4], Step [108/1006], loss: 1.1384\n",
      "Epoch [2/4], Step [112/1006], loss: 0.9410\n",
      "Epoch [2/4], Step [116/1006], loss: 0.2425\n",
      "Epoch [2/4], Step [120/1006], loss: 0.2101\n",
      "Epoch [2/4], Step [124/1006], loss: 1.1933\n",
      "Epoch [2/4], Step [128/1006], loss: 0.5085\n",
      "Epoch [2/4], Step [132/1006], loss: 0.2597\n",
      "Epoch [2/4], Step [136/1006], loss: 0.2231\n",
      "Epoch [2/4], Step [140/1006], loss: 1.4832\n",
      "Epoch [2/4], Step [144/1006], loss: 1.0249\n",
      "Epoch [2/4], Step [148/1006], loss: 0.2166\n",
      "Epoch [2/4], Step [152/1006], loss: 1.9158\n",
      "Epoch [2/4], Step [156/1006], loss: 0.5309\n",
      "Epoch [2/4], Step [160/1006], loss: 0.4066\n",
      "Epoch [2/4], Step [164/1006], loss: 1.1176\n",
      "Epoch [2/4], Step [168/1006], loss: 0.2199\n",
      "Epoch [2/4], Step [172/1006], loss: 0.8359\n",
      "Epoch [2/4], Step [176/1006], loss: 1.8371\n",
      "Epoch [2/4], Step [180/1006], loss: 1.2723\n",
      "Epoch [2/4], Step [184/1006], loss: 0.7507\n",
      "Epoch [2/4], Step [188/1006], loss: 0.6189\n",
      "Epoch [2/4], Step [192/1006], loss: 2.0381\n",
      "Epoch [2/4], Step [196/1006], loss: 0.3525\n",
      "Epoch [2/4], Step [200/1006], loss: 1.6874\n",
      "Epoch [2/4], Step [204/1006], loss: 2.2495\n",
      "Epoch [2/4], Step [208/1006], loss: 0.9502\n",
      "Epoch [2/4], Step [212/1006], loss: 0.5938\n",
      "Epoch [2/4], Step [216/1006], loss: 0.6423\n",
      "Epoch [2/4], Step [220/1006], loss: 0.5629\n",
      "Epoch [2/4], Step [224/1006], loss: 0.5929\n",
      "Epoch [2/4], Step [228/1006], loss: 0.5778\n",
      "Epoch [2/4], Step [232/1006], loss: 0.3990\n",
      "Epoch [2/4], Step [236/1006], loss: 3.0396\n",
      "Epoch [2/4], Step [240/1006], loss: 0.1314\n",
      "Epoch [2/4], Step [244/1006], loss: 2.0508\n",
      "Epoch [2/4], Step [248/1006], loss: 0.7426\n",
      "Epoch [2/4], Step [252/1006], loss: 0.3906\n",
      "Epoch [2/4], Step [256/1006], loss: 0.7553\n",
      "Epoch [2/4], Step [260/1006], loss: 0.7011\n",
      "Epoch [2/4], Step [264/1006], loss: 2.2158\n",
      "Epoch [2/4], Step [268/1006], loss: 0.7239\n",
      "Epoch [2/4], Step [272/1006], loss: 0.1091\n",
      "Epoch [2/4], Step [276/1006], loss: 1.9089\n",
      "Epoch [2/4], Step [280/1006], loss: 1.4022\n",
      "Epoch [2/4], Step [284/1006], loss: 0.3305\n",
      "Epoch [2/4], Step [288/1006], loss: 0.9753\n",
      "Epoch [2/4], Step [292/1006], loss: 2.0116\n",
      "Epoch [2/4], Step [296/1006], loss: 0.3942\n",
      "Epoch [2/4], Step [300/1006], loss: 0.2615\n",
      "Epoch [2/4], Step [304/1006], loss: 1.0383\n",
      "Epoch [2/4], Step [308/1006], loss: 0.5897\n",
      "Epoch [2/4], Step [312/1006], loss: 1.5809\n",
      "Epoch [2/4], Step [316/1006], loss: 1.1664\n",
      "Epoch [2/4], Step [320/1006], loss: 1.1898\n",
      "Epoch [2/4], Step [324/1006], loss: 0.9146\n",
      "Epoch [2/4], Step [328/1006], loss: 0.3717\n",
      "Epoch [2/4], Step [332/1006], loss: 2.3598\n",
      "Epoch [2/4], Step [336/1006], loss: 0.3954\n",
      "Epoch [2/4], Step [340/1006], loss: 1.4961\n",
      "Epoch [2/4], Step [344/1006], loss: 0.9092\n",
      "Epoch [2/4], Step [348/1006], loss: 1.3296\n",
      "Epoch [2/4], Step [352/1006], loss: 1.1430\n",
      "Epoch [2/4], Step [356/1006], loss: 0.3275\n",
      "Epoch [2/4], Step [360/1006], loss: 1.3619\n",
      "Epoch [2/4], Step [364/1006], loss: 0.3357\n",
      "Epoch [2/4], Step [368/1006], loss: 0.5315\n",
      "Epoch [2/4], Step [372/1006], loss: 0.9251\n",
      "Epoch [2/4], Step [376/1006], loss: 0.6005\n",
      "Epoch [2/4], Step [380/1006], loss: 0.5746\n",
      "Epoch [2/4], Step [384/1006], loss: 0.5449\n",
      "Epoch [2/4], Step [388/1006], loss: 1.1939\n",
      "Epoch [2/4], Step [392/1006], loss: 0.9313\n",
      "Epoch [2/4], Step [396/1006], loss: 1.2932\n",
      "Epoch [2/4], Step [400/1006], loss: 1.6214\n",
      "Epoch [2/4], Step [404/1006], loss: 0.7387\n",
      "Epoch [2/4], Step [408/1006], loss: 1.6404\n",
      "Epoch [2/4], Step [412/1006], loss: 1.3631\n",
      "Epoch [2/4], Step [416/1006], loss: 0.4343\n",
      "Epoch [2/4], Step [420/1006], loss: 0.7653\n",
      "Epoch [2/4], Step [424/1006], loss: 0.2634\n",
      "Epoch [2/4], Step [428/1006], loss: 1.4487\n",
      "Epoch [2/4], Step [432/1006], loss: 0.3197\n",
      "Epoch [2/4], Step [436/1006], loss: 1.4181\n",
      "Epoch [2/4], Step [440/1006], loss: 0.1142\n",
      "Epoch [2/4], Step [444/1006], loss: 0.5385\n",
      "Epoch [2/4], Step [448/1006], loss: 1.3798\n",
      "Epoch [2/4], Step [452/1006], loss: 0.4245\n",
      "Epoch [2/4], Step [456/1006], loss: 0.7330\n",
      "Epoch [2/4], Step [460/1006], loss: 0.1295\n",
      "Epoch [2/4], Step [464/1006], loss: 0.7511\n",
      "Epoch [2/4], Step [468/1006], loss: 0.5412\n",
      "Epoch [2/4], Step [472/1006], loss: 0.5192\n",
      "Epoch [2/4], Step [476/1006], loss: 0.3927\n",
      "Epoch [2/4], Step [480/1006], loss: 0.7224\n",
      "Epoch [2/4], Step [484/1006], loss: 0.3645\n",
      "Epoch [2/4], Step [488/1006], loss: 0.3490\n",
      "Epoch [2/4], Step [492/1006], loss: 0.6495\n",
      "Epoch [2/4], Step [496/1006], loss: 1.8186\n",
      "Epoch [2/4], Step [500/1006], loss: 0.6128\n",
      "Epoch [2/4], Step [504/1006], loss: 0.6605\n",
      "Epoch [2/4], Step [508/1006], loss: 0.9338\n",
      "Epoch [2/4], Step [512/1006], loss: 0.4916\n",
      "Epoch [2/4], Step [516/1006], loss: 0.4006\n",
      "Epoch [2/4], Step [520/1006], loss: 0.1003\n",
      "Epoch [2/4], Step [524/1006], loss: 0.5388\n",
      "Epoch [2/4], Step [528/1006], loss: 0.3642\n",
      "Epoch [2/4], Step [532/1006], loss: 1.5861\n",
      "Epoch [2/4], Step [536/1006], loss: 1.2927\n",
      "Epoch [2/4], Step [540/1006], loss: 0.2633\n",
      "Epoch [2/4], Step [544/1006], loss: 0.9829\n",
      "Epoch [2/4], Step [548/1006], loss: 1.4732\n",
      "Epoch [2/4], Step [552/1006], loss: 0.1916\n",
      "Epoch [2/4], Step [556/1006], loss: 0.9272\n",
      "Epoch [2/4], Step [560/1006], loss: 0.1463\n",
      "Epoch [2/4], Step [564/1006], loss: 0.2678\n",
      "Epoch [2/4], Step [568/1006], loss: 0.3248\n",
      "Epoch [2/4], Step [572/1006], loss: 0.4180\n",
      "Epoch [2/4], Step [576/1006], loss: 1.6832\n",
      "Epoch [2/4], Step [580/1006], loss: 0.2725\n",
      "Epoch [2/4], Step [584/1006], loss: 0.9575\n",
      "Epoch [2/4], Step [588/1006], loss: 0.3074\n",
      "Epoch [2/4], Step [592/1006], loss: 0.1837\n",
      "Epoch [2/4], Step [596/1006], loss: 0.6066\n",
      "Epoch [2/4], Step [600/1006], loss: 0.2656\n",
      "Epoch [2/4], Step [604/1006], loss: 0.3699\n",
      "Epoch [2/4], Step [608/1006], loss: 0.3822\n",
      "Epoch [2/4], Step [612/1006], loss: 1.8600\n",
      "Epoch [2/4], Step [616/1006], loss: 1.1844\n",
      "Epoch [2/4], Step [620/1006], loss: 1.0334\n",
      "Epoch [2/4], Step [624/1006], loss: 0.3574\n",
      "Epoch [2/4], Step [628/1006], loss: 0.3156\n",
      "Epoch [2/4], Step [632/1006], loss: 0.3356\n",
      "Epoch [2/4], Step [636/1006], loss: 0.1443\n",
      "Epoch [2/4], Step [640/1006], loss: 0.6454\n",
      "Epoch [2/4], Step [644/1006], loss: 0.0775\n",
      "Epoch [2/4], Step [648/1006], loss: 0.2765\n",
      "Epoch [2/4], Step [652/1006], loss: 3.0596\n",
      "Epoch [2/4], Step [656/1006], loss: 0.2760\n",
      "Epoch [2/4], Step [660/1006], loss: 2.7433\n",
      "Epoch [2/4], Step [664/1006], loss: 1.0191\n",
      "Epoch [2/4], Step [668/1006], loss: 0.7115\n",
      "Epoch [2/4], Step [672/1006], loss: 0.4140\n",
      "Epoch [2/4], Step [676/1006], loss: 0.8158\n",
      "Epoch [2/4], Step [680/1006], loss: 0.1148\n",
      "Epoch [2/4], Step [684/1006], loss: 0.9015\n",
      "Epoch [2/4], Step [688/1006], loss: 0.7047\n",
      "Epoch [2/4], Step [692/1006], loss: 1.6328\n",
      "Epoch [2/4], Step [696/1006], loss: 0.5553\n",
      "Epoch [2/4], Step [700/1006], loss: 1.3891\n",
      "Epoch [2/4], Step [704/1006], loss: 0.7943\n",
      "Epoch [2/4], Step [708/1006], loss: 1.0073\n",
      "Epoch [2/4], Step [712/1006], loss: 1.3330\n",
      "Epoch [2/4], Step [716/1006], loss: 1.0312\n",
      "Epoch [2/4], Step [720/1006], loss: 0.8728\n",
      "Epoch [2/4], Step [724/1006], loss: 0.5844\n",
      "Epoch [2/4], Step [728/1006], loss: 0.6602\n",
      "Epoch [2/4], Step [732/1006], loss: 0.1464\n",
      "Epoch [2/4], Step [736/1006], loss: 0.2752\n",
      "Epoch [2/4], Step [740/1006], loss: 0.1462\n",
      "Epoch [2/4], Step [744/1006], loss: 1.0989\n",
      "Epoch [2/4], Step [748/1006], loss: 0.8862\n",
      "Epoch [2/4], Step [752/1006], loss: 0.1237\n",
      "Epoch [2/4], Step [756/1006], loss: 0.6539\n",
      "Epoch [2/4], Step [760/1006], loss: 1.9088\n",
      "Epoch [2/4], Step [764/1006], loss: 0.6672\n",
      "Epoch [2/4], Step [768/1006], loss: 0.4106\n",
      "Epoch [2/4], Step [772/1006], loss: 1.2592\n",
      "Epoch [2/4], Step [776/1006], loss: 0.3747\n",
      "Epoch [2/4], Step [780/1006], loss: 0.4177\n",
      "Epoch [2/4], Step [784/1006], loss: 0.4744\n",
      "Epoch [2/4], Step [788/1006], loss: 1.4252\n",
      "Epoch [2/4], Step [792/1006], loss: 0.5788\n",
      "Epoch [2/4], Step [796/1006], loss: 0.8325\n",
      "Epoch [2/4], Step [800/1006], loss: 0.1687\n",
      "Epoch [2/4], Step [804/1006], loss: 1.5703\n",
      "Epoch [2/4], Step [808/1006], loss: 1.1296\n",
      "Epoch [2/4], Step [812/1006], loss: 0.7160\n",
      "Epoch [2/4], Step [816/1006], loss: 0.3542\n",
      "Epoch [2/4], Step [820/1006], loss: 0.7867\n",
      "Epoch [2/4], Step [824/1006], loss: 1.6352\n",
      "Epoch [2/4], Step [828/1006], loss: 2.4470\n",
      "Epoch [2/4], Step [832/1006], loss: 0.1555\n",
      "Epoch [2/4], Step [836/1006], loss: 0.4869\n",
      "Epoch [2/4], Step [840/1006], loss: 0.7157\n",
      "Epoch [2/4], Step [844/1006], loss: 0.5362\n",
      "Epoch [2/4], Step [848/1006], loss: 0.9492\n",
      "Epoch [2/4], Step [852/1006], loss: 1.4529\n",
      "Epoch [2/4], Step [856/1006], loss: 0.2777\n",
      "Epoch [2/4], Step [860/1006], loss: 1.7746\n",
      "Epoch [2/4], Step [864/1006], loss: 0.2210\n",
      "Epoch [2/4], Step [868/1006], loss: 0.3851\n",
      "Epoch [2/4], Step [872/1006], loss: 0.4018\n",
      "Epoch [2/4], Step [876/1006], loss: 0.6326\n",
      "Epoch [2/4], Step [880/1006], loss: 0.8677\n",
      "Epoch [2/4], Step [884/1006], loss: 0.9741\n",
      "Epoch [2/4], Step [888/1006], loss: 0.6055\n",
      "Epoch [2/4], Step [892/1006], loss: 0.4957\n",
      "Epoch [2/4], Step [896/1006], loss: 0.2624\n",
      "Epoch [2/4], Step [900/1006], loss: 0.2463\n",
      "Epoch [2/4], Step [904/1006], loss: 1.2391\n",
      "Epoch [2/4], Step [908/1006], loss: 0.3587\n",
      "Epoch [2/4], Step [912/1006], loss: 0.0482\n",
      "Epoch [2/4], Step [916/1006], loss: 1.2448\n",
      "Epoch [2/4], Step [920/1006], loss: 1.2154\n",
      "Epoch [2/4], Step [924/1006], loss: 1.8406\n",
      "Epoch [2/4], Step [928/1006], loss: 1.0557\n",
      "Epoch [2/4], Step [932/1006], loss: 0.1750\n",
      "Epoch [2/4], Step [936/1006], loss: 0.6097\n",
      "Epoch [2/4], Step [940/1006], loss: 0.7579\n",
      "Epoch [2/4], Step [944/1006], loss: 0.3115\n",
      "Epoch [2/4], Step [948/1006], loss: 1.5418\n",
      "Epoch [2/4], Step [952/1006], loss: 0.6548\n",
      "Epoch [2/4], Step [956/1006], loss: 1.5908\n",
      "Epoch [2/4], Step [960/1006], loss: 1.2781\n",
      "Epoch [2/4], Step [964/1006], loss: 0.4323\n",
      "Epoch [2/4], Step [968/1006], loss: 0.1488\n",
      "Epoch [2/4], Step [972/1006], loss: 1.0260\n",
      "Epoch [2/4], Step [976/1006], loss: 1.1040\n",
      "Epoch [2/4], Step [980/1006], loss: 0.9003\n",
      "Epoch [2/4], Step [984/1006], loss: 0.8043\n",
      "Epoch [2/4], Step [988/1006], loss: 0.9316\n",
      "Epoch [2/4], Step [992/1006], loss: 2.0902\n",
      "Epoch [2/4], Step [996/1006], loss: 1.3884\n",
      "Epoch [2/4], Step [1000/1006], loss: 0.1245\n",
      "Epoch [2/4], Step [1004/1006], loss: 0.1693\n",
      "Epoch [3/4], Step [4/1006], loss: 0.7267\n",
      "Epoch [3/4], Step [8/1006], loss: 0.9001\n",
      "Epoch [3/4], Step [12/1006], loss: 0.5246\n",
      "Epoch [3/4], Step [16/1006], loss: 0.1913\n",
      "Epoch [3/4], Step [20/1006], loss: 0.6825\n",
      "Epoch [3/4], Step [24/1006], loss: 2.4016\n",
      "Epoch [3/4], Step [28/1006], loss: 0.5306\n",
      "Epoch [3/4], Step [32/1006], loss: 0.5957\n",
      "Epoch [3/4], Step [36/1006], loss: 0.0309\n",
      "Epoch [3/4], Step [40/1006], loss: 1.0903\n",
      "Epoch [3/4], Step [44/1006], loss: 0.5979\n",
      "Epoch [3/4], Step [48/1006], loss: 1.2788\n",
      "Epoch [3/4], Step [52/1006], loss: 0.8584\n",
      "Epoch [3/4], Step [56/1006], loss: 0.8092\n",
      "Epoch [3/4], Step [60/1006], loss: 2.2203\n",
      "Epoch [3/4], Step [64/1006], loss: 0.5086\n",
      "Epoch [3/4], Step [68/1006], loss: 0.8549\n",
      "Epoch [3/4], Step [72/1006], loss: 0.6787\n",
      "Epoch [3/4], Step [76/1006], loss: 0.7341\n",
      "Epoch [3/4], Step [80/1006], loss: 1.0839\n",
      "Epoch [3/4], Step [84/1006], loss: 0.4952\n",
      "Epoch [3/4], Step [88/1006], loss: 0.7425\n",
      "Epoch [3/4], Step [92/1006], loss: 0.5898\n",
      "Epoch [3/4], Step [96/1006], loss: 0.6838\n",
      "Epoch [3/4], Step [100/1006], loss: 0.2162\n",
      "Epoch [3/4], Step [104/1006], loss: 0.4104\n",
      "Epoch [3/4], Step [108/1006], loss: 0.9487\n",
      "Epoch [3/4], Step [112/1006], loss: 0.8266\n",
      "Epoch [3/4], Step [116/1006], loss: 0.1212\n",
      "Epoch [3/4], Step [120/1006], loss: 0.0102\n",
      "Epoch [3/4], Step [124/1006], loss: 1.0078\n",
      "Epoch [3/4], Step [128/1006], loss: 0.5348\n",
      "Epoch [3/4], Step [132/1006], loss: 0.0548\n",
      "Epoch [3/4], Step [136/1006], loss: 0.0618\n",
      "Epoch [3/4], Step [140/1006], loss: 1.0542\n",
      "Epoch [3/4], Step [144/1006], loss: 0.8310\n",
      "Epoch [3/4], Step [148/1006], loss: 0.1059\n",
      "Epoch [3/4], Step [152/1006], loss: 1.4092\n",
      "Epoch [3/4], Step [156/1006], loss: 0.5067\n",
      "Epoch [3/4], Step [160/1006], loss: 0.4569\n",
      "Epoch [3/4], Step [164/1006], loss: 0.8542\n",
      "Epoch [3/4], Step [168/1006], loss: 0.2118\n",
      "Epoch [3/4], Step [172/1006], loss: 0.6461\n",
      "Epoch [3/4], Step [176/1006], loss: 1.7160\n",
      "Epoch [3/4], Step [180/1006], loss: 1.6333\n",
      "Epoch [3/4], Step [184/1006], loss: 0.5064\n",
      "Epoch [3/4], Step [188/1006], loss: 0.6130\n",
      "Epoch [3/4], Step [192/1006], loss: 1.7445\n",
      "Epoch [3/4], Step [196/1006], loss: 0.2417\n",
      "Epoch [3/4], Step [200/1006], loss: 1.3096\n",
      "Epoch [3/4], Step [204/1006], loss: 2.5590\n",
      "Epoch [3/4], Step [208/1006], loss: 0.6165\n",
      "Epoch [3/4], Step [212/1006], loss: 0.6263\n",
      "Epoch [3/4], Step [216/1006], loss: 0.9294\n",
      "Epoch [3/4], Step [220/1006], loss: 0.3815\n",
      "Epoch [3/4], Step [224/1006], loss: 0.4868\n",
      "Epoch [3/4], Step [228/1006], loss: 0.5498\n",
      "Epoch [3/4], Step [232/1006], loss: 0.3619\n",
      "Epoch [3/4], Step [236/1006], loss: 2.2424\n",
      "Epoch [3/4], Step [240/1006], loss: 0.0132\n",
      "Epoch [3/4], Step [244/1006], loss: 1.8109\n",
      "Epoch [3/4], Step [248/1006], loss: 0.6892\n",
      "Epoch [3/4], Step [252/1006], loss: 0.2541\n",
      "Epoch [3/4], Step [256/1006], loss: 0.6932\n",
      "Epoch [3/4], Step [260/1006], loss: 0.8562\n",
      "Epoch [3/4], Step [264/1006], loss: 1.8238\n",
      "Epoch [3/4], Step [268/1006], loss: 0.7559\n",
      "Epoch [3/4], Step [272/1006], loss: 0.1586\n",
      "Epoch [3/4], Step [276/1006], loss: 1.5695\n",
      "Epoch [3/4], Step [280/1006], loss: 0.9621\n",
      "Epoch [3/4], Step [284/1006], loss: 0.3913\n",
      "Epoch [3/4], Step [288/1006], loss: 0.8928\n",
      "Epoch [3/4], Step [292/1006], loss: 2.2654\n",
      "Epoch [3/4], Step [296/1006], loss: 0.3502\n",
      "Epoch [3/4], Step [300/1006], loss: 0.2508\n",
      "Epoch [3/4], Step [304/1006], loss: 0.7866\n",
      "Epoch [3/4], Step [308/1006], loss: 0.7654\n",
      "Epoch [3/4], Step [312/1006], loss: 1.2725\n",
      "Epoch [3/4], Step [316/1006], loss: 1.0352\n",
      "Epoch [3/4], Step [320/1006], loss: 0.7624\n",
      "Epoch [3/4], Step [324/1006], loss: 0.6904\n",
      "Epoch [3/4], Step [328/1006], loss: 0.3168\n",
      "Epoch [3/4], Step [332/1006], loss: 2.0402\n",
      "Epoch [3/4], Step [336/1006], loss: 0.2743\n",
      "Epoch [3/4], Step [340/1006], loss: 1.0974\n",
      "Epoch [3/4], Step [344/1006], loss: 0.7209\n",
      "Epoch [3/4], Step [348/1006], loss: 1.1122\n",
      "Epoch [3/4], Step [352/1006], loss: 0.7414\n",
      "Epoch [3/4], Step [356/1006], loss: 0.3794\n",
      "Epoch [3/4], Step [360/1006], loss: 1.0670\n",
      "Epoch [3/4], Step [364/1006], loss: 0.3676\n",
      "Epoch [3/4], Step [368/1006], loss: 0.6554\n",
      "Epoch [3/4], Step [372/1006], loss: 0.6528\n",
      "Epoch [3/4], Step [376/1006], loss: 0.5482\n",
      "Epoch [3/4], Step [380/1006], loss: 0.7130\n",
      "Epoch [3/4], Step [384/1006], loss: 0.5222\n",
      "Epoch [3/4], Step [388/1006], loss: 0.7619\n",
      "Epoch [3/4], Step [392/1006], loss: 0.9873\n",
      "Epoch [3/4], Step [396/1006], loss: 1.6600\n",
      "Epoch [3/4], Step [400/1006], loss: 1.4509\n",
      "Epoch [3/4], Step [404/1006], loss: 0.6972\n",
      "Epoch [3/4], Step [408/1006], loss: 1.0920\n",
      "Epoch [3/4], Step [412/1006], loss: 1.3737\n",
      "Epoch [3/4], Step [416/1006], loss: 0.4098\n",
      "Epoch [3/4], Step [420/1006], loss: 0.6626\n",
      "Epoch [3/4], Step [424/1006], loss: 0.2073\n",
      "Epoch [3/4], Step [428/1006], loss: 1.1253\n",
      "Epoch [3/4], Step [432/1006], loss: 0.4447\n",
      "Epoch [3/4], Step [436/1006], loss: 1.2673\n",
      "Epoch [3/4], Step [440/1006], loss: 0.1708\n",
      "Epoch [3/4], Step [444/1006], loss: 0.4086\n",
      "Epoch [3/4], Step [448/1006], loss: 1.5510\n",
      "Epoch [3/4], Step [452/1006], loss: 0.2156\n",
      "Epoch [3/4], Step [456/1006], loss: 0.4322\n",
      "Epoch [3/4], Step [460/1006], loss: 0.1095\n",
      "Epoch [3/4], Step [464/1006], loss: 1.0375\n",
      "Epoch [3/4], Step [468/1006], loss: 0.6151\n",
      "Epoch [3/4], Step [472/1006], loss: 0.6967\n",
      "Epoch [3/4], Step [476/1006], loss: 0.5155\n",
      "Epoch [3/4], Step [480/1006], loss: 0.5202\n",
      "Epoch [3/4], Step [484/1006], loss: 0.4193\n",
      "Epoch [3/4], Step [488/1006], loss: 0.2828\n",
      "Epoch [3/4], Step [492/1006], loss: 0.4849\n",
      "Epoch [3/4], Step [496/1006], loss: 1.3890\n",
      "Epoch [3/4], Step [500/1006], loss: 0.7640\n",
      "Epoch [3/4], Step [504/1006], loss: 0.5999\n",
      "Epoch [3/4], Step [508/1006], loss: 1.1189\n",
      "Epoch [3/4], Step [512/1006], loss: 0.3105\n",
      "Epoch [3/4], Step [516/1006], loss: 0.3539\n",
      "Epoch [3/4], Step [520/1006], loss: 0.0040\n",
      "Epoch [3/4], Step [524/1006], loss: 0.4274\n",
      "Epoch [3/4], Step [528/1006], loss: 0.3056\n",
      "Epoch [3/4], Step [532/1006], loss: 1.3812\n",
      "Epoch [3/4], Step [536/1006], loss: 0.8876\n",
      "Epoch [3/4], Step [540/1006], loss: 0.1134\n",
      "Epoch [3/4], Step [544/1006], loss: 0.8664\n",
      "Epoch [3/4], Step [548/1006], loss: 1.3850\n",
      "Epoch [3/4], Step [552/1006], loss: 0.1375\n",
      "Epoch [3/4], Step [556/1006], loss: 0.9882\n",
      "Epoch [3/4], Step [560/1006], loss: 0.2902\n",
      "Epoch [3/4], Step [564/1006], loss: 0.2240\n",
      "Epoch [3/4], Step [568/1006], loss: 0.3876\n",
      "Epoch [3/4], Step [572/1006], loss: 0.4480\n",
      "Epoch [3/4], Step [576/1006], loss: 1.6886\n",
      "Epoch [3/4], Step [580/1006], loss: 0.3312\n",
      "Epoch [3/4], Step [584/1006], loss: 0.6545\n",
      "Epoch [3/4], Step [588/1006], loss: 0.2448\n",
      "Epoch [3/4], Step [592/1006], loss: 0.1693\n",
      "Epoch [3/4], Step [596/1006], loss: 0.4990\n",
      "Epoch [3/4], Step [600/1006], loss: 0.2059\n",
      "Epoch [3/4], Step [604/1006], loss: 0.5062\n",
      "Epoch [3/4], Step [608/1006], loss: 0.4521\n",
      "Epoch [3/4], Step [612/1006], loss: 1.4488\n",
      "Epoch [3/4], Step [616/1006], loss: 0.9476\n",
      "Epoch [3/4], Step [620/1006], loss: 0.7346\n",
      "Epoch [3/4], Step [624/1006], loss: 0.4500\n",
      "Epoch [3/4], Step [628/1006], loss: 0.3426\n",
      "Epoch [3/4], Step [632/1006], loss: 0.3887\n",
      "Epoch [3/4], Step [636/1006], loss: 0.0407\n",
      "Epoch [3/4], Step [640/1006], loss: 0.5450\n",
      "Epoch [3/4], Step [644/1006], loss: 0.1089\n",
      "Epoch [3/4], Step [648/1006], loss: 0.3559\n",
      "Epoch [3/4], Step [652/1006], loss: 2.7541\n",
      "Epoch [3/4], Step [656/1006], loss: 0.2720\n",
      "Epoch [3/4], Step [660/1006], loss: 2.2803\n",
      "Epoch [3/4], Step [664/1006], loss: 1.0474\n",
      "Epoch [3/4], Step [668/1006], loss: 0.4633\n",
      "Epoch [3/4], Step [672/1006], loss: 0.4403\n",
      "Epoch [3/4], Step [676/1006], loss: 0.7318\n",
      "Epoch [3/4], Step [680/1006], loss: 0.0169\n",
      "Epoch [3/4], Step [684/1006], loss: 1.1582\n",
      "Epoch [3/4], Step [688/1006], loss: 0.9915\n",
      "Epoch [3/4], Step [692/1006], loss: 1.2764\n",
      "Epoch [3/4], Step [696/1006], loss: 0.6349\n",
      "Epoch [3/4], Step [700/1006], loss: 1.0116\n",
      "Epoch [3/4], Step [704/1006], loss: 1.0849\n",
      "Epoch [3/4], Step [708/1006], loss: 0.9108\n",
      "Epoch [3/4], Step [712/1006], loss: 0.9991\n",
      "Epoch [3/4], Step [716/1006], loss: 0.9398\n",
      "Epoch [3/4], Step [720/1006], loss: 0.8463\n",
      "Epoch [3/4], Step [724/1006], loss: 0.8167\n",
      "Epoch [3/4], Step [728/1006], loss: 0.8829\n",
      "Epoch [3/4], Step [732/1006], loss: 0.0272\n",
      "Epoch [3/4], Step [736/1006], loss: 0.3295\n",
      "Epoch [3/4], Step [740/1006], loss: 0.0362\n",
      "Epoch [3/4], Step [744/1006], loss: 1.0299\n",
      "Epoch [3/4], Step [748/1006], loss: 0.6391\n",
      "Epoch [3/4], Step [752/1006], loss: 0.3465\n",
      "Epoch [3/4], Step [756/1006], loss: 0.4887\n",
      "Epoch [3/4], Step [760/1006], loss: 1.5931\n",
      "Epoch [3/4], Step [764/1006], loss: 0.5610\n",
      "Epoch [3/4], Step [768/1006], loss: 0.2951\n",
      "Epoch [3/4], Step [772/1006], loss: 1.0975\n",
      "Epoch [3/4], Step [776/1006], loss: 0.3253\n",
      "Epoch [3/4], Step [780/1006], loss: 0.3438\n",
      "Epoch [3/4], Step [784/1006], loss: 0.5456\n",
      "Epoch [3/4], Step [788/1006], loss: 1.0391\n",
      "Epoch [3/4], Step [792/1006], loss: 0.5704\n",
      "Epoch [3/4], Step [796/1006], loss: 0.8529\n",
      "Epoch [3/4], Step [800/1006], loss: 0.2095\n",
      "Epoch [3/4], Step [804/1006], loss: 1.4415\n",
      "Epoch [3/4], Step [808/1006], loss: 0.9928\n",
      "Epoch [3/4], Step [812/1006], loss: 0.8679\n",
      "Epoch [3/4], Step [816/1006], loss: 0.3779\n",
      "Epoch [3/4], Step [820/1006], loss: 1.0176\n",
      "Epoch [3/4], Step [824/1006], loss: 1.6379\n",
      "Epoch [3/4], Step [828/1006], loss: 2.3867\n",
      "Epoch [3/4], Step [832/1006], loss: 0.0662\n",
      "Epoch [3/4], Step [836/1006], loss: 0.6666\n",
      "Epoch [3/4], Step [840/1006], loss: 0.6116\n",
      "Epoch [3/4], Step [844/1006], loss: 0.6104\n",
      "Epoch [3/4], Step [848/1006], loss: 0.8485\n",
      "Epoch [3/4], Step [852/1006], loss: 1.4372\n",
      "Epoch [3/4], Step [856/1006], loss: 0.4060\n",
      "Epoch [3/4], Step [860/1006], loss: 1.4166\n",
      "Epoch [3/4], Step [864/1006], loss: 0.2847\n",
      "Epoch [3/4], Step [868/1006], loss: 0.2493\n",
      "Epoch [3/4], Step [872/1006], loss: 0.3917\n",
      "Epoch [3/4], Step [876/1006], loss: 0.5566\n",
      "Epoch [3/4], Step [880/1006], loss: 0.8012\n",
      "Epoch [3/4], Step [884/1006], loss: 0.9304\n",
      "Epoch [3/4], Step [888/1006], loss: 0.8240\n",
      "Epoch [3/4], Step [892/1006], loss: 0.5410\n",
      "Epoch [3/4], Step [896/1006], loss: 0.3238\n",
      "Epoch [3/4], Step [900/1006], loss: 0.1558\n",
      "Epoch [3/4], Step [904/1006], loss: 1.0400\n",
      "Epoch [3/4], Step [908/1006], loss: 0.3881\n",
      "Epoch [3/4], Step [912/1006], loss: 0.0279\n",
      "Epoch [3/4], Step [916/1006], loss: 0.9167\n",
      "Epoch [3/4], Step [920/1006], loss: 1.2643\n",
      "Epoch [3/4], Step [924/1006], loss: 1.8653\n",
      "Epoch [3/4], Step [928/1006], loss: 1.1317\n",
      "Epoch [3/4], Step [932/1006], loss: 0.1674\n",
      "Epoch [3/4], Step [936/1006], loss: 0.6394\n",
      "Epoch [3/4], Step [940/1006], loss: 0.8497\n",
      "Epoch [3/4], Step [944/1006], loss: 0.1577\n",
      "Epoch [3/4], Step [948/1006], loss: 1.4596\n",
      "Epoch [3/4], Step [952/1006], loss: 0.4919\n",
      "Epoch [3/4], Step [956/1006], loss: 1.2436\n",
      "Epoch [3/4], Step [960/1006], loss: 1.1145\n",
      "Epoch [3/4], Step [964/1006], loss: 0.4711\n",
      "Epoch [3/4], Step [968/1006], loss: 0.0599\n",
      "Epoch [3/4], Step [972/1006], loss: 1.2294\n",
      "Epoch [3/4], Step [976/1006], loss: 1.3019\n",
      "Epoch [3/4], Step [980/1006], loss: 1.1139\n",
      "Epoch [3/4], Step [984/1006], loss: 0.9302\n",
      "Epoch [3/4], Step [988/1006], loss: 0.6321\n",
      "Epoch [3/4], Step [992/1006], loss: 1.9520\n",
      "Epoch [3/4], Step [996/1006], loss: 1.1343\n",
      "Epoch [3/4], Step [1000/1006], loss: 0.1304\n",
      "Epoch [3/4], Step [1004/1006], loss: 0.1284\n",
      "Epoch [4/4], Step [4/1006], loss: 0.6335\n",
      "Epoch [4/4], Step [8/1006], loss: 1.0498\n",
      "Epoch [4/4], Step [12/1006], loss: 0.4457\n",
      "Epoch [4/4], Step [16/1006], loss: 0.1852\n",
      "Epoch [4/4], Step [20/1006], loss: 0.7456\n",
      "Epoch [4/4], Step [24/1006], loss: 2.3163\n",
      "Epoch [4/4], Step [28/1006], loss: 0.6296\n",
      "Epoch [4/4], Step [32/1006], loss: 0.6071\n",
      "Epoch [4/4], Step [36/1006], loss: 0.0212\n",
      "Epoch [4/4], Step [40/1006], loss: 0.9558\n",
      "Epoch [4/4], Step [44/1006], loss: 0.4415\n",
      "Epoch [4/4], Step [48/1006], loss: 1.1783\n",
      "Epoch [4/4], Step [52/1006], loss: 0.7009\n",
      "Epoch [4/4], Step [56/1006], loss: 0.5522\n",
      "Epoch [4/4], Step [60/1006], loss: 2.1139\n",
      "Epoch [4/4], Step [64/1006], loss: 0.3603\n",
      "Epoch [4/4], Step [68/1006], loss: 0.6180\n",
      "Epoch [4/4], Step [72/1006], loss: 0.8643\n",
      "Epoch [4/4], Step [76/1006], loss: 0.8533\n",
      "Epoch [4/4], Step [80/1006], loss: 1.3465\n",
      "Epoch [4/4], Step [84/1006], loss: 0.3703\n",
      "Epoch [4/4], Step [88/1006], loss: 0.7910\n",
      "Epoch [4/4], Step [92/1006], loss: 0.4252\n",
      "Epoch [4/4], Step [96/1006], loss: 0.8581\n",
      "Epoch [4/4], Step [100/1006], loss: 0.1157\n",
      "Epoch [4/4], Step [104/1006], loss: 0.3636\n",
      "Epoch [4/4], Step [108/1006], loss: 0.9091\n",
      "Epoch [4/4], Step [112/1006], loss: 0.7890\n",
      "Epoch [4/4], Step [116/1006], loss: 0.0739\n",
      "Epoch [4/4], Step [120/1006], loss: 0.0091\n",
      "Epoch [4/4], Step [124/1006], loss: 0.9169\n",
      "Epoch [4/4], Step [128/1006], loss: 0.5586\n",
      "Epoch [4/4], Step [132/1006], loss: 0.0071\n",
      "Epoch [4/4], Step [136/1006], loss: 0.0195\n",
      "Epoch [4/4], Step [140/1006], loss: 0.8503\n",
      "Epoch [4/4], Step [144/1006], loss: 0.7328\n",
      "Epoch [4/4], Step [148/1006], loss: 0.1247\n",
      "Epoch [4/4], Step [152/1006], loss: 1.1435\n",
      "Epoch [4/4], Step [156/1006], loss: 0.4919\n",
      "Epoch [4/4], Step [160/1006], loss: 0.4996\n",
      "Epoch [4/4], Step [164/1006], loss: 0.7655\n",
      "Epoch [4/4], Step [168/1006], loss: 0.2059\n",
      "Epoch [4/4], Step [172/1006], loss: 0.6128\n",
      "Epoch [4/4], Step [176/1006], loss: 1.6567\n",
      "Epoch [4/4], Step [180/1006], loss: 1.8830\n",
      "Epoch [4/4], Step [184/1006], loss: 0.3876\n",
      "Epoch [4/4], Step [188/1006], loss: 0.5849\n",
      "Epoch [4/4], Step [192/1006], loss: 1.6341\n",
      "Epoch [4/4], Step [196/1006], loss: 0.2038\n",
      "Epoch [4/4], Step [200/1006], loss: 1.1177\n",
      "Epoch [4/4], Step [204/1006], loss: 2.7603\n",
      "Epoch [4/4], Step [208/1006], loss: 0.4713\n",
      "Epoch [4/4], Step [212/1006], loss: 0.6660\n",
      "Epoch [4/4], Step [216/1006], loss: 1.1238\n",
      "Epoch [4/4], Step [220/1006], loss: 0.2981\n",
      "Epoch [4/4], Step [224/1006], loss: 0.4305\n",
      "Epoch [4/4], Step [228/1006], loss: 0.5528\n",
      "Epoch [4/4], Step [232/1006], loss: 0.4042\n",
      "Epoch [4/4], Step [236/1006], loss: 1.7921\n",
      "Epoch [4/4], Step [240/1006], loss: 0.0165\n",
      "Epoch [4/4], Step [244/1006], loss: 1.6713\n",
      "Epoch [4/4], Step [248/1006], loss: 0.6765\n",
      "Epoch [4/4], Step [252/1006], loss: 0.1960\n",
      "Epoch [4/4], Step [256/1006], loss: 0.6544\n",
      "Epoch [4/4], Step [260/1006], loss: 0.9791\n",
      "Epoch [4/4], Step [264/1006], loss: 1.6158\n",
      "Epoch [4/4], Step [268/1006], loss: 0.7581\n",
      "Epoch [4/4], Step [272/1006], loss: 0.1887\n",
      "Epoch [4/4], Step [276/1006], loss: 1.3936\n",
      "Epoch [4/4], Step [280/1006], loss: 0.7401\n",
      "Epoch [4/4], Step [284/1006], loss: 0.4596\n",
      "Epoch [4/4], Step [288/1006], loss: 0.8585\n",
      "Epoch [4/4], Step [292/1006], loss: 2.4498\n",
      "Epoch [4/4], Step [296/1006], loss: 0.3246\n",
      "Epoch [4/4], Step [300/1006], loss: 0.2579\n",
      "Epoch [4/4], Step [304/1006], loss: 0.6564\n",
      "Epoch [4/4], Step [308/1006], loss: 0.8712\n",
      "Epoch [4/4], Step [312/1006], loss: 1.0946\n",
      "Epoch [4/4], Step [316/1006], loss: 1.0140\n",
      "Epoch [4/4], Step [320/1006], loss: 0.5510\n",
      "Epoch [4/4], Step [324/1006], loss: 0.5742\n",
      "Epoch [4/4], Step [328/1006], loss: 0.3401\n",
      "Epoch [4/4], Step [332/1006], loss: 1.8741\n",
      "Epoch [4/4], Step [336/1006], loss: 0.2358\n",
      "Epoch [4/4], Step [340/1006], loss: 0.8877\n",
      "Epoch [4/4], Step [344/1006], loss: 0.6166\n",
      "Epoch [4/4], Step [348/1006], loss: 1.0099\n",
      "Epoch [4/4], Step [352/1006], loss: 0.5717\n",
      "Epoch [4/4], Step [356/1006], loss: 0.4054\n",
      "Epoch [4/4], Step [360/1006], loss: 0.9100\n",
      "Epoch [4/4], Step [364/1006], loss: 0.4075\n",
      "Epoch [4/4], Step [368/1006], loss: 0.7469\n",
      "Epoch [4/4], Step [372/1006], loss: 0.5121\n",
      "Epoch [4/4], Step [376/1006], loss: 0.5264\n",
      "Epoch [4/4], Step [380/1006], loss: 0.8007\n",
      "Epoch [4/4], Step [384/1006], loss: 0.5108\n",
      "Epoch [4/4], Step [388/1006], loss: 0.5411\n",
      "Epoch [4/4], Step [392/1006], loss: 1.0482\n",
      "Epoch [4/4], Step [396/1006], loss: 1.8261\n",
      "Epoch [4/4], Step [400/1006], loss: 1.3660\n",
      "Epoch [4/4], Step [404/1006], loss: 0.6862\n",
      "Epoch [4/4], Step [408/1006], loss: 0.8068\n",
      "Epoch [4/4], Step [412/1006], loss: 1.4099\n",
      "Epoch [4/4], Step [416/1006], loss: 0.4147\n",
      "Epoch [4/4], Step [420/1006], loss: 0.6046\n",
      "Epoch [4/4], Step [424/1006], loss: 0.2040\n",
      "Epoch [4/4], Step [428/1006], loss: 0.9474\n",
      "Epoch [4/4], Step [432/1006], loss: 0.5229\n",
      "Epoch [4/4], Step [436/1006], loss: 1.2113\n",
      "Epoch [4/4], Step [440/1006], loss: 0.2343\n",
      "Epoch [4/4], Step [444/1006], loss: 0.3733\n",
      "Epoch [4/4], Step [448/1006], loss: 1.6801\n",
      "Epoch [4/4], Step [452/1006], loss: 0.1462\n",
      "Epoch [4/4], Step [456/1006], loss: 0.3168\n",
      "Epoch [4/4], Step [460/1006], loss: 0.1282\n",
      "Epoch [4/4], Step [464/1006], loss: 1.2285\n",
      "Epoch [4/4], Step [468/1006], loss: 0.6312\n",
      "Epoch [4/4], Step [472/1006], loss: 0.8264\n",
      "Epoch [4/4], Step [476/1006], loss: 0.5860\n",
      "Epoch [4/4], Step [480/1006], loss: 0.4231\n",
      "Epoch [4/4], Step [484/1006], loss: 0.4453\n",
      "Epoch [4/4], Step [488/1006], loss: 0.2594\n",
      "Epoch [4/4], Step [492/1006], loss: 0.3947\n",
      "Epoch [4/4], Step [496/1006], loss: 1.1640\n",
      "Epoch [4/4], Step [500/1006], loss: 0.8516\n",
      "Epoch [4/4], Step [504/1006], loss: 0.5828\n",
      "Epoch [4/4], Step [508/1006], loss: 1.2494\n",
      "Epoch [4/4], Step [512/1006], loss: 0.2262\n",
      "Epoch [4/4], Step [516/1006], loss: 0.3610\n",
      "Epoch [4/4], Step [520/1006], loss: 0.0012\n",
      "Epoch [4/4], Step [524/1006], loss: 0.3692\n",
      "Epoch [4/4], Step [528/1006], loss: 0.2920\n",
      "Epoch [4/4], Step [532/1006], loss: 1.2742\n",
      "Epoch [4/4], Step [536/1006], loss: 0.6910\n",
      "Epoch [4/4], Step [540/1006], loss: 0.0685\n",
      "Epoch [4/4], Step [544/1006], loss: 0.8186\n",
      "Epoch [4/4], Step [548/1006], loss: 1.3406\n",
      "Epoch [4/4], Step [552/1006], loss: 0.1043\n",
      "Epoch [4/4], Step [556/1006], loss: 1.0368\n",
      "Epoch [4/4], Step [560/1006], loss: 0.3450\n",
      "Epoch [4/4], Step [564/1006], loss: 0.2140\n",
      "Epoch [4/4], Step [568/1006], loss: 0.4173\n",
      "Epoch [4/4], Step [572/1006], loss: 0.5072\n",
      "Epoch [4/4], Step [576/1006], loss: 1.7049\n",
      "Epoch [4/4], Step [580/1006], loss: 0.3954\n",
      "Epoch [4/4], Step [584/1006], loss: 0.5183\n",
      "Epoch [4/4], Step [588/1006], loss: 0.2081\n",
      "Epoch [4/4], Step [592/1006], loss: 0.1723\n",
      "Epoch [4/4], Step [596/1006], loss: 0.4705\n",
      "Epoch [4/4], Step [600/1006], loss: 0.1768\n",
      "Epoch [4/4], Step [604/1006], loss: 0.5939\n",
      "Epoch [4/4], Step [608/1006], loss: 0.4887\n",
      "Epoch [4/4], Step [612/1006], loss: 1.2438\n",
      "Epoch [4/4], Step [616/1006], loss: 0.8528\n",
      "Epoch [4/4], Step [620/1006], loss: 0.6143\n",
      "Epoch [4/4], Step [624/1006], loss: 0.4723\n",
      "Epoch [4/4], Step [628/1006], loss: 0.3547\n",
      "Epoch [4/4], Step [632/1006], loss: 0.4217\n",
      "Epoch [4/4], Step [636/1006], loss: 0.0171\n",
      "Epoch [4/4], Step [640/1006], loss: 0.4919\n",
      "Epoch [4/4], Step [644/1006], loss: 0.1393\n",
      "Epoch [4/4], Step [648/1006], loss: 0.3697\n",
      "Epoch [4/4], Step [652/1006], loss: 2.6141\n",
      "Epoch [4/4], Step [656/1006], loss: 0.2890\n",
      "Epoch [4/4], Step [660/1006], loss: 2.0429\n",
      "Epoch [4/4], Step [664/1006], loss: 1.0780\n",
      "Epoch [4/4], Step [668/1006], loss: 0.3602\n",
      "Epoch [4/4], Step [672/1006], loss: 0.4859\n",
      "Epoch [4/4], Step [676/1006], loss: 0.7039\n",
      "Epoch [4/4], Step [680/1006], loss: 0.0063\n",
      "Epoch [4/4], Step [684/1006], loss: 1.3110\n",
      "Epoch [4/4], Step [688/1006], loss: 1.1589\n",
      "Epoch [4/4], Step [692/1006], loss: 1.1104\n",
      "Epoch [4/4], Step [696/1006], loss: 0.6925\n",
      "Epoch [4/4], Step [700/1006], loss: 0.8373\n",
      "Epoch [4/4], Step [704/1006], loss: 1.2522\n",
      "Epoch [4/4], Step [708/1006], loss: 0.8690\n",
      "Epoch [4/4], Step [712/1006], loss: 0.8478\n",
      "Epoch [4/4], Step [716/1006], loss: 0.8923\n",
      "Epoch [4/4], Step [720/1006], loss: 0.8405\n",
      "Epoch [4/4], Step [724/1006], loss: 0.9487\n",
      "Epoch [4/4], Step [728/1006], loss: 1.0055\n",
      "Epoch [4/4], Step [732/1006], loss: 0.0123\n",
      "Epoch [4/4], Step [736/1006], loss: 0.3500\n",
      "Epoch [4/4], Step [740/1006], loss: 0.0169\n",
      "Epoch [4/4], Step [744/1006], loss: 1.0161\n",
      "Epoch [4/4], Step [748/1006], loss: 0.5283\n",
      "Epoch [4/4], Step [752/1006], loss: 0.3540\n",
      "Epoch [4/4], Step [756/1006], loss: 0.4107\n",
      "Epoch [4/4], Step [760/1006], loss: 1.4507\n",
      "Epoch [4/4], Step [764/1006], loss: 0.5191\n",
      "Epoch [4/4], Step [768/1006], loss: 0.2409\n",
      "Epoch [4/4], Step [772/1006], loss: 1.0595\n",
      "Epoch [4/4], Step [776/1006], loss: 0.3040\n",
      "Epoch [4/4], Step [780/1006], loss: 0.2947\n",
      "Epoch [4/4], Step [784/1006], loss: 0.5675\n",
      "Epoch [4/4], Step [788/1006], loss: 0.8469\n",
      "Epoch [4/4], Step [792/1006], loss: 0.5679\n",
      "Epoch [4/4], Step [796/1006], loss: 0.8327\n",
      "Epoch [4/4], Step [800/1006], loss: 0.2268\n",
      "Epoch [4/4], Step [804/1006], loss: 1.3749\n",
      "Epoch [4/4], Step [808/1006], loss: 0.9284\n",
      "Epoch [4/4], Step [812/1006], loss: 0.9460\n",
      "Epoch [4/4], Step [816/1006], loss: 0.4087\n",
      "Epoch [4/4], Step [820/1006], loss: 1.0989\n",
      "Epoch [4/4], Step [824/1006], loss: 1.6482\n",
      "Epoch [4/4], Step [828/1006], loss: 2.4020\n",
      "Epoch [4/4], Step [832/1006], loss: 0.0540\n",
      "Epoch [4/4], Step [836/1006], loss: 0.7731\n",
      "Epoch [4/4], Step [840/1006], loss: 0.5767\n",
      "Epoch [4/4], Step [844/1006], loss: 0.6529\n",
      "Epoch [4/4], Step [848/1006], loss: 0.8020\n",
      "Epoch [4/4], Step [852/1006], loss: 1.4372\n",
      "Epoch [4/4], Step [856/1006], loss: 0.4764\n",
      "Epoch [4/4], Step [860/1006], loss: 1.2498\n",
      "Epoch [4/4], Step [864/1006], loss: 0.3110\n",
      "Epoch [4/4], Step [868/1006], loss: 0.1916\n",
      "Epoch [4/4], Step [872/1006], loss: 0.3994\n",
      "Epoch [4/4], Step [876/1006], loss: 0.4992\n",
      "Epoch [4/4], Step [880/1006], loss: 0.7683\n",
      "Epoch [4/4], Step [884/1006], loss: 0.9249\n",
      "Epoch [4/4], Step [888/1006], loss: 0.9604\n",
      "Epoch [4/4], Step [892/1006], loss: 0.5748\n",
      "Epoch [4/4], Step [896/1006], loss: 0.3341\n",
      "Epoch [4/4], Step [900/1006], loss: 0.1188\n",
      "Epoch [4/4], Step [904/1006], loss: 0.9549\n",
      "Epoch [4/4], Step [908/1006], loss: 0.3915\n",
      "Epoch [4/4], Step [912/1006], loss: 0.0268\n",
      "Epoch [4/4], Step [916/1006], loss: 0.7574\n",
      "Epoch [4/4], Step [920/1006], loss: 1.3073\n",
      "Epoch [4/4], Step [924/1006], loss: 1.8808\n",
      "Epoch [4/4], Step [928/1006], loss: 1.1954\n",
      "Epoch [4/4], Step [932/1006], loss: 0.1881\n",
      "Epoch [4/4], Step [936/1006], loss: 0.6682\n",
      "Epoch [4/4], Step [940/1006], loss: 0.9077\n",
      "Epoch [4/4], Step [944/1006], loss: 0.1145\n",
      "Epoch [4/4], Step [948/1006], loss: 1.4320\n",
      "Epoch [4/4], Step [952/1006], loss: 0.4333\n",
      "Epoch [4/4], Step [956/1006], loss: 1.0480\n",
      "Epoch [4/4], Step [960/1006], loss: 1.0333\n",
      "Epoch [4/4], Step [964/1006], loss: 0.5061\n",
      "Epoch [4/4], Step [968/1006], loss: 0.0421\n",
      "Epoch [4/4], Step [972/1006], loss: 1.3304\n",
      "Epoch [4/4], Step [976/1006], loss: 1.3287\n",
      "Epoch [4/4], Step [980/1006], loss: 1.1542\n",
      "Epoch [4/4], Step [984/1006], loss: 0.9846\n",
      "Epoch [4/4], Step [988/1006], loss: 0.4874\n",
      "Epoch [4/4], Step [992/1006], loss: 1.9187\n",
      "Epoch [4/4], Step [996/1006], loss: 1.0307\n",
      "Epoch [4/4], Step [1000/1006], loss: 0.1294\n",
      "Epoch [4/4], Step [1004/1006], loss: 0.1042\n"
     ]
    }
   ],
   "source": [
    "# Training our model:\n",
    "\n",
    "n_total_steps = len(trainloader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if (i+1) % 4 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f67c5e6-f643-49fc-92c4-0d249c269a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 504 test closing prices: 1.1905 %\n",
      "outputs: tensor([[-0.6427],\n",
      "        [-0.5553]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      " labels: tensor([[-0.8844],\n",
      "        [-0.4102]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = len(testloader.dataset)\n",
    "    for inputs, labels in testloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        outputs = model(inputs)\n",
    "        a, predicted = torch.max(outputs, 1)\n",
    "        n_correct += (abs(predicted - labels) < 0.02).sum().item()\n",
    "    acc = n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the {n_samples} test closing prices: {100*acc:.4f} %')\n",
    "\n",
    "for inputs, labels in testloader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    print(f\"outputs: {outputs}\\n labels: {labels}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7badcc-2194-46f5-8f9c-a7cc4ee9aeb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
